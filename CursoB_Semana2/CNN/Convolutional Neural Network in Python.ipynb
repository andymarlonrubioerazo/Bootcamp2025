{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network TensorFlow in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: Convolutional Neural Networks (CNN)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Basic Concepts](#basic-concepts)\n",
    "   - 2.1 [What is a CNN?](#what-is-a-cnn)\n",
    "   - 2.2 [Applications of CNNs](#applications-of-cnns)\n",
    "3. [Components of a CNN](#components-of-a-cnn)\n",
    "   - 3.1 [Convolution](#convolution)\n",
    "   - 3.2 [Activation Functions](#activation-functions)\n",
    "   - 3.3 [Pooling](#pooling)\n",
    "   - 3.4 [Fully Connected Layers](#fully-connected-layers)\n",
    "   - 3.5 [Loss Function and Optimization](#loss-function-and-optimization)\n",
    "4. [Neural network techniques](#techniques)  \n",
    "   - 4.1 [Activations Functions](#activation)\n",
    "   - 4.2 [Overfitting  and Early stoping](#stoping)\n",
    "   - 4.3 [Optimizer](#optimizer)\n",
    "5. [Seting up the environment](#SETTING)\n",
    "   - 5.1 [Installing the packages](#instal)\n",
    "6. [CNN assembling - MNIST](#CNN)\n",
    "   - 6.1 [A siple CNN architecture](#architecture)\n",
    "   - 6.2 [Preprocesing the data](#preprocesing)\n",
    "   - 6.3 [Building and training the CNN](#train)\n",
    "   - 6.4 [Testing the trained CNN](#testing)\n",
    "7. [Tensorboard: Visualization  tool for TensorFlow](#tensorflow)\n",
    "   - 7.1 [Tensorboard on the MINST example](#example)\n",
    "   - 7.2 [Visualization Confusion Matriz with Tensorboard](#confusion)\n",
    "   - 7.3 [Using tensorboard to tune hyperparameters](#hyperparameter)\n",
    "8. [Common techniques for better performance of neural networks ](#techniques)\n",
    "   - 8.1 [Regularization](#regu)\n",
    "   - 8.2 [L2 Regularization and weight decay](#L2)\n",
    "   - 8.3 [Dropout](#dropout)\n",
    "   - 8.4 [Data Augmentation](#dataaugmentatio)\n",
    " 9. [A practical project: Labelling fashion items](#example)\n",
    "   - 9.1 [Introduction to the problem](#intro)\n",
    "   - 9.2 [The objective and the image]($objective)\n",
    "   - 9.3 [Converting images to array](#array)\n",
    "   - 9.4 [Coding](#code)\n",
    "   - 9.5 [Classification](#classification)\n",
    "    \n",
    "10. [Unexpectet failures](#failures)\n",
    "11. [Classical CNN Architectures](#classical-architectures)\n",
    "\n",
    "\n",
    "# 1. [Introduction](#introduction)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have revolutionized deep learning, especially in image processing tasks. Since their introduction in the 1990s, CNNs have become a fundamental tool for problems like object recognition, image segmentation, and computer vision.\n",
    "\n",
    "In this course, we will explore the theoretical and practical foundations of CNNs, covering from basic concepts to the implementation of modern architectures using Python and PyTorch. We will also address model training, optimization, and strategies for improving performance.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. [Basic Concepts](#basic-concepts)\n",
    "\n",
    "### What is a CNN?\n",
    "\n",
    "A Convolutional Neural Network (CNN) is a specialized type of neural network designed to process grid-like data structures, such as images. CNNs are particularly effective at recognizing patterns and features in images by applying mathematical operations called convolutions.\n",
    "\n",
    "### Applications of CNNs\n",
    "\n",
    "CNNs are widely used in:\n",
    "- **Image recognition**: Classifying objects and people in images.\n",
    "- **Video processing**: Real-time object detection.\n",
    "- **Speech recognition**: Applications in virtual assistants like Siri and Alexa.\n",
    "- **Medical diagnostics**: Analyzing medical images for tumor detection.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. [Components of a CNN](#components-of-a-cnn)\n",
    "\n",
    "### Convolution\n",
    "\n",
    "Convolution is the key operation in a CNN. This operation takes a filter (kernel) and slides it over the input image to extract important features such as edges, textures, and shapes.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearities into the model, allowing the network to learn more complex relationships. The most common is the **ReLU** (Rectified Linear Unit) function.\n",
    "\n",
    "### Pooling\n",
    "\n",
    "**Pooling** reduces the dimensionality of the feature maps, which decreases the computational load. Common types include **Max Pooling** and **Average Pooling**.\n",
    "\n",
    "### Fully Connected Layers\n",
    "\n",
    "After feature extraction with convolutional layers, the data is passed through fully connected layers to make the final classification.\n",
    "\n",
    "### Loss Function and Optimization\n",
    "\n",
    "The **loss function** measures how far off the network’s predictions are from the true values. An optimizer like **Adam** or **SGD** is used to adjust the model’s weights to minimize this loss.\n",
    "\n",
    "---\n",
    "\n",
    "## Classical Architectures\n",
    "\n",
    "### LeNet\n",
    "\n",
    "LeNet was one of the first CNNs developed to recognize handwritten digits in grayscale images.\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "AlexNet was a major breakthrough in 2012, introducing a deep CNN design that won the ImageNet competition.\n",
    "\n",
    "### VGGNet\n",
    "\n",
    "VGGNet introduced a simple yet deep CNN design with many small convolutional layers stacked together.\n",
    "\n",
    "### ResNet\n",
    "\n",
    "ResNet introduced the idea of residual connections, allowing extremely deep networks to be trained without gradient degradation issues.\n",
    "\n",
    "---\n",
    "\n",
    "## Training a CNN\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "Before training a CNN, proper data preparation is crucial, which includes tasks such as normalization, data augmentation, and splitting into training and test sets.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters, such as learning rate, kernel size, and the number of filters, must be fine-tuned to optimize the model’s performance.\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "It is important to evaluate the CNN’s performance using metrics like accuracy, confusion matrix, and ROC curve.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implementation with Python\n",
    "\n",
    "### Building a CNN from Scratch with PyTorch\n",
    "\n",
    "Here, we will implement a basic CNN using PyTorch. This will cover key steps such as defining the model, loss function, optimizer, and training the model.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64*6*6, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64*6*6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. [Neural network techniques](#techniques) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 [Activation funtions](#activation-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![e](re.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions in Neural Networks\n",
    "\n",
    "In neural networks, activation functions introduce non-linearities that allow the network to model complex relationships in data. Below are the most common activation functions and their roles:\n",
    "\n",
    "## 1. Sigmoid (Logistic) Function\n",
    "**Formula**: \n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- **Range**: (0, 1)\n",
    "- **Description**: The sigmoid function maps input values to a range between 0 and 1. It is commonly used in binary classification tasks.\n",
    "- **Use Case**: It is often used in the output layer for binary classification problems, where the output can be interpreted as a probability.\n",
    "- **Pros**: Smooth gradient, useful for models that need probability outputs.\n",
    "- **Cons**: Prone to vanishing gradient problems, especially in deeper networks, because of its flat gradients for extreme values.\n",
    "\n",
    "## 2. Tanh (Hyperbolic Tangent)\n",
    "**Formula**:\n",
    "$$\n",
    "\\tanh(x) = \\frac{2}{1 + e^{-2x}} - 1\n",
    "$$\n",
    "\n",
    "- **Range**: (-1, 1)\n",
    "- **Description**: The tanh function is similar to the sigmoid but its output range is between -1 and 1, which centers the data better for further layers.\n",
    "- **Use Case**: Like the sigmoid, but preferred over sigmoid for hidden layers since it produces stronger gradients and helps mitigate the vanishing gradient problem.\n",
    "- **Pros**: Zero-centered output makes optimization easier compared to sigmoid.\n",
    "- **Cons**: Still susceptible to vanishing gradients for extreme inputs.\n",
    "\n",
    "## 3. ReLU (Rectified Linear Unit)\n",
    "**Formula**:\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "- **Range**: [0, ∞)\n",
    "- **Description**: ReLU is a piecewise linear function that outputs zero if the input is negative and outputs the input itself if positive. This allows the network to ignore irrelevant data and focus on important signals.\n",
    "- **Use Case**: Widely used in hidden layers of neural networks because it helps with faster convergence during training.\n",
    "- **Pros**: Helps mitigate the vanishing gradient problem, simple and efficient.\n",
    "- **Cons**: Can lead to \"dead neurons\" where neurons stop activating altogether if they only receive negative inputs (this is known as the **Dying ReLU Problem**).\n",
    "\n",
    "## 4. Leaky ReLU\n",
    "**Formula**:\n",
    "$$\n",
    "\\text{Leaky ReLU}(x) = \\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Range**: (-∞, ∞)\n",
    "- **Description**: Leaky ReLU allows for a small, non-zero gradient for negative inputs to solve the \"Dying ReLU\" problem. The parameter \\(\\alpha\\) is typically a small constant (e.g., 0.01).\n",
    "- **Use Case**: Used in deep networks to prevent dead neurons.\n",
    "- **Pros**: Retains the benefits of ReLU while avoiding the dead neuron issue.\n",
    "- **Cons**: The choice of \\(\\alpha\\) is arbitrary and may require tuning.\n",
    "\n",
    "## 5. ELU (Exponential Linear Unit)\n",
    "**Formula**:\n",
    "$$\n",
    "\\text{ELU}(x) = \\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (e^x - 1) & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Range**: (-α, ∞)\n",
    "- **Description**: ELU is similar to Leaky ReLU but smoother, which improves learning characteristics. For negative inputs, it has an exponential growth, controlled by the parameter \\(\\alpha\\).\n",
    "- **Use Case**: Often used in deep neural networks to reduce bias shifts and improve learning speed.\n",
    "- **Pros**: Helps in reducing vanishing gradient problems, with better smoothing for negative values.\n",
    "- **Cons**: More computationally expensive than ReLU.\n",
    "\n",
    "## 6. Softmax\n",
    "**Formula**:\n",
    "$$\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "\n",
    "- **Range**: (0, 1) for each output, sum = 1\n",
    "- **Description**: Softmax is often used in the output layer of classification problems where multiple classes exist. It converts raw scores (logits) into probabilities, where the probabilities of all classes sum to 1.\n",
    "- **Use Case**: Used in multi-class classification tasks.\n",
    "- **Pros**: Provides a probabilistic interpretation for outputs.\n",
    "- **Cons**: Susceptible to vanishing gradient issues if not used carefully.\n",
    "\n",
    "## 7. Swish\n",
    "**Formula**:\n",
    "$$\n",
    "\\text{Swish}(x) = \\frac{x}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- **Range**: (-∞, ∞)\n",
    "- **Description**: Swish is a smooth, non-monotonic activation function that has been shown to perform better than ReLU on deeper networks.\n",
    "- **Use Case**: Used in deep neural networks for better gradient flow.\n",
    "- **Pros**: Helps improve model accuracy on deep networks compared to ReLU.\n",
    "- **Cons**: Slightly more computationally expensive than ReLU.\n",
    "\n",
    "## 8. GELU (Gaussian Error Linear Unit)\n",
    "**Formula**:\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$$\n",
    "where $\\Phi(x)$ is the cumulative distribution function of a Gaussian distribution.\n",
    "\n",
    "- **Range**: (-∞, ∞)\n",
    "- **Description**: GELU is a smoother version of ReLU, approximating a Gaussian curve. It can better handle small positive inputs than ReLU.\n",
    "- **Use Case**: GELU is often used in transformer architectures like BERT.\n",
    "- **Pros**: Better performance in certain tasks compared to ReLU or Swish.\n",
    "- **Cons**: More computationally expensive than ReLU.\n",
    "\n",
    "## 9. SELU (Scaled Exponential Linear Unit)\n",
    "**Formula**:\n",
    "$$\n",
    "\\text{SELU}(x) = \\lambda \\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (e^x - 1) & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Range**: (-∞, ∞)\n",
    "- **Description**: SELU self-normalizes the output of each layer, leading to better convergence without the need for explicit normalization layers. It’s similar to ELU but with a scaling factor $\\lambda$.\n",
    "- **Use Case**: Used in self-normalizing networks (SNNs).\n",
    "- **Pros**: Automatic self-normalization helps with stable training.\n",
    "- **Cons**: Requires careful initialization and constraints on network architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 [Overfitting  and Early stoping](#stoping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![er](erly.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overfitting\n",
    "Overfitting occurs when a machine learning model becomes too complex, learning noise and details from the training data that don't generalize well to unseen data. This typically happens when the model fits the training data very well but performs poorly on test data.\n",
    "\n",
    "### Causes of Overfitting:\n",
    "- **Too many parameters** in the model relative to the amount of training data.\n",
    "- **Too few training examples** or an imbalance in the dataset.\n",
    "- **Overly complex models**, such as very deep neural networks, that have too much capacity to memorize the training data.\n",
    "\n",
    "### Signs of Overfitting:\n",
    "- Very low error on the training set but high error on the test/validation set.\n",
    "- Large gap between training and test performance.\n",
    "\n",
    "## Early Stopping {#stopping}\n",
    "Early stopping is a regularization technique that helps prevent overfitting in machine learning models. The idea is to monitor the model’s performance on a validation set during training and stop the training process once the performance on the validation set begins to degrade.\n",
    "\n",
    "### How Early Stopping Works:\n",
    "1. **Monitor validation loss**: Keep track of how well the model performs on the validation set at the end of each epoch.\n",
    "2. **Stop when performance worsens**: If the validation loss starts increasing after a certain point, it indicates that the model is beginning to overfit.\n",
    "3. **Save the best model**: The model parameters at the epoch with the best validation loss are saved and used as the final model.\n",
    "\n",
    "### Advantages of Early Stopping:\n",
    "- Prevents overfitting and improves generalization on unseen data.\n",
    "- Reduces training time by halting the process before unnecessary epochs.\n",
    "- Simple to implement and commonly used in deep learning models.\n",
    "\n",
    "### Practical Implementation\n",
    "Most deep learning libraries, like TensorFlow and PyTorch, provide easy mechanisms for early stopping, usually by specifying a validation set and a patience parameter (how many epochs to wait before stopping).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4.3 [Optimizer](#optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Optimizer\n",
    "Optimizers are algorithms or methods used to change the attributes of the neural network, such as weights and learning rates, to reduce the losses. They are key components in training neural networks.\n",
    "\n",
    "### Types of Optimizers:\n",
    "\n",
    "#### 1. **Stochastic Gradient Descent (SGD)**\n",
    "- **Description**: SGD updates the weights by using the gradient of the loss function with respect to a single data point at a time. It helps in navigating towards the global minimum of the cost function.\n",
    "- **Pros**: Faster convergence for large datasets.\n",
    "- **Cons**: Noisy updates due to one data point being used for weight updates, which can make convergence harder.\n",
    "  \n",
    "#### 2. **Mini-batch Gradient Descent**\n",
    "- **Description**: A compromise between batch gradient descent and SGD. It uses a small batch of data points for each iteration instead of the whole dataset (batch) or a single point (SGD).\n",
    "- **Pros**: Provides a balance between speed and stability, improving the performance of the optimizer.\n",
    "- **Cons**: Requires careful tuning of batch size.\n",
    "\n",
    "#### 3. **Momentum**\n",
    "- **Description**: Momentum helps accelerate the gradient descent by using the past gradients to smooth the update path. It adds a fraction of the previous update to the current one.\n",
    "- **Pros**: Helps in faster convergence and reduces oscillations in directions of steep curvature.\n",
    "- **Cons**: Introduces another hyperparameter (momentum coefficient) that needs tuning.\n",
    "\n",
    "#### 4. **AdaGrad**\n",
    "- **Description**: AdaGrad adapts the learning rate to each individual parameter, making large updates for infrequent parameters and small updates for frequent parameters.\n",
    "- **Pros**: Works well with sparse data.\n",
    "- **Cons**: Learning rate can shrink too much, stopping learning prematurely.\n",
    "\n",
    "#### 5. **RMSprop**\n",
    "- **Description**: RMSprop adjusts the learning rate by dividing it by an exponentially decaying average of squared gradients. It helps overcome the diminishing learning rate issue of AdaGrad.\n",
    "- **Pros**: Suitable for non-stationary problems, where the learning process changes over time.\n",
    "- **Cons**: Still requires tuning the learning rate.\n",
    "\n",
    "#### 6. **Adam (Adaptive Moment Estimation)**\n",
    "- **Description**: Combines the advantages of both AdaGrad and RMSprop. Adam keeps track of an exponentially decaying average of past gradients and the squared gradients.\n",
    "- **Pros**: Efficient and works well with noisy data, requiring less tuning of the learning rate.\n",
    "- **Cons**: Can converge to suboptimal solutions in some cases.\n",
    "\n",
    "### Choosing the Right Optimizer:\n",
    "- **SGD with momentum** is often a good default choice for many problems.\n",
    "- **Adam** is typically preferred for deep learning tasks due to its adaptability and efficiency.\n",
    "- **RMSprop** can be used for recurrent neural networks or other non-stationary problems.\n",
    "\n",
    "### Learning Rate Scheduling:\n",
    "It's important to adjust the learning rate during training for better performance. Some common scheduling techniques include:\n",
    "- **Step decay**: Reduces the learning rate by a factor at specific intervals.\n",
    "- **Exponential decay**: Reduces the learning rate exponentially after each epoch.\n",
    "- **Learning rate annealing**: Gradually decreases the learning rate over time.\n",
    "\n",
    "```python\n",
    "# Example in Keras (TensorFlow)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. [Seting up the environment](#SETTING)\n",
    "   ## - 5.1 [Installing the packages](#instal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping tensorflow as it is not installed.\n",
      "WARNING: Skipping tensorflow-intel as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall tensorflow tensorflow-intel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 12\n"
     ]
    }
   ],
   "source": [
    "! pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.17.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting tensorflow-intel==2.17.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (74.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)\n",
      "Requirement already satisfied: namex in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.17.0-cp311-cp311-win_amd64.whl (2.0 kB)\n",
      "Downloading tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl (385.0 MB)\n",
      "   ---------------------------------------- 0.0/385.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/385.0 MB 16.8 MB/s eta 0:00:23\n",
      "    --------------------------------------- 8.4/385.0 MB 27.5 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 16.8/385.0 MB 32.1 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 23.1/385.0 MB 31.8 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 29.1/385.0 MB 30.8 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 34.9/385.0 MB 30.4 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 39.8/385.0 MB 29.5 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 45.6/385.0 MB 29.3 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 50.6/385.0 MB 28.5 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 56.4/385.0 MB 28.5 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 63.2/385.0 MB 29.0 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 69.7/385.0 MB 29.2 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 76.3/385.0 MB 29.3 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 82.3/385.0 MB 29.3 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 88.6/385.0 MB 29.4 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 94.9/385.0 MB 29.5 MB/s eta 0:00:10\n",
      "   ---------- ---------------------------- 101.4/385.0 MB 29.8 MB/s eta 0:00:10\n",
      "   ---------- ---------------------------- 105.4/385.0 MB 29.4 MB/s eta 0:00:10\n",
      "   ----------- --------------------------- 109.3/385.0 MB 28.6 MB/s eta 0:00:10\n",
      "   ----------- --------------------------- 113.0/385.0 MB 28.0 MB/s eta 0:00:10\n",
      "   ----------- --------------------------- 115.9/385.0 MB 27.4 MB/s eta 0:00:10\n",
      "   ------------ -------------------------- 118.8/385.0 MB 26.9 MB/s eta 0:00:10\n",
      "   ------------ -------------------------- 122.4/385.0 MB 26.3 MB/s eta 0:00:10\n",
      "   ------------ -------------------------- 125.0/385.0 MB 26.1 MB/s eta 0:00:10\n",
      "   ------------- ------------------------- 128.5/385.0 MB 25.3 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 132.1/385.0 MB 25.1 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 135.5/385.0 MB 24.7 MB/s eta 0:00:11\n",
      "   -------------- ------------------------ 138.4/385.0 MB 24.4 MB/s eta 0:00:11\n",
      "   -------------- ------------------------ 142.6/385.0 MB 24.2 MB/s eta 0:00:11\n",
      "   -------------- ------------------------ 146.3/385.0 MB 24.0 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 150.2/385.0 MB 23.8 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 154.4/385.0 MB 23.7 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 158.3/385.0 MB 23.6 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 160.4/385.0 MB 23.2 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 162.5/385.0 MB 22.8 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 167.0/385.0 MB 22.8 MB/s eta 0:00:10\n",
      "   ----------------- --------------------- 171.2/385.0 MB 22.7 MB/s eta 0:00:10\n",
      "   ----------------- --------------------- 174.3/385.0 MB 22.5 MB/s eta 0:00:10\n",
      "   ------------------ -------------------- 177.7/385.0 MB 22.4 MB/s eta 0:00:10\n",
      "   ------------------ -------------------- 180.9/385.0 MB 22.2 MB/s eta 0:00:10\n",
      "   ------------------ -------------------- 185.1/385.0 MB 22.1 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 189.3/385.0 MB 22.1 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 193.2/385.0 MB 22.0 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 197.1/385.0 MB 22.0 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 201.3/385.0 MB 21.9 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 205.3/385.0 MB 21.8 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 209.5/385.0 MB 21.8 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 211.8/385.0 MB 21.6 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 214.7/385.0 MB 21.4 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 217.6/385.0 MB 21.3 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 220.7/385.0 MB 21.1 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 223.6/385.0 MB 21.0 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 227.0/385.0 MB 20.9 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 229.6/385.0 MB 20.8 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 232.8/385.0 MB 20.6 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 235.1/385.0 MB 20.5 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 237.5/385.0 MB 20.3 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 240.1/385.0 MB 20.2 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 242.7/385.0 MB 20.1 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 245.4/385.0 MB 19.9 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 248.0/385.0 MB 19.8 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 250.6/385.0 MB 19.7 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 253.5/385.0 MB 19.6 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 255.9/385.0 MB 19.5 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 258.7/385.0 MB 19.4 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 261.6/385.0 MB 19.3 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 264.5/385.0 MB 19.2 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 267.4/385.0 MB 19.0 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 270.3/385.0 MB 18.9 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 273.2/385.0 MB 18.7 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 276.0/385.0 MB 18.6 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 278.4/385.0 MB 18.3 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 279.7/385.0 MB 18.1 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 281.3/385.0 MB 17.9 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 282.6/385.0 MB 17.7 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 284.2/385.0 MB 17.6 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 285.7/385.0 MB 17.4 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 287.3/385.0 MB 17.2 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 288.9/385.0 MB 17.0 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 290.7/385.0 MB 16.9 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 292.3/385.0 MB 16.7 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 294.1/385.0 MB 16.6 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 295.7/385.0 MB 16.4 MB/s eta 0:00:06\n",
      "   ------------------------------ -------- 297.5/385.0 MB 16.3 MB/s eta 0:00:06\n",
      "   ------------------------------ -------- 299.4/385.0 MB 16.1 MB/s eta 0:00:06\n",
      "   ------------------------------ -------- 301.2/385.0 MB 16.0 MB/s eta 0:00:06\n",
      "   ------------------------------ -------- 303.0/385.0 MB 15.9 MB/s eta 0:00:06\n",
      "   ------------------------------ -------- 304.6/385.0 MB 15.8 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 306.4/385.0 MB 15.6 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 308.5/385.0 MB 15.5 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 310.6/385.0 MB 15.4 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 312.5/385.0 MB 15.3 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 314.6/385.0 MB 15.2 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 316.7/385.0 MB 15.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 318.8/385.0 MB 14.9 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 321.1/385.0 MB 14.8 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 323.2/385.0 MB 14.7 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 325.6/385.0 MB 14.6 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 327.7/385.0 MB 14.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 329.8/385.0 MB 14.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 332.1/385.0 MB 14.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 334.5/385.0 MB 14.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 336.9/385.0 MB 14.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 339.2/385.0 MB 14.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 341.8/385.0 MB 13.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 343.9/385.0 MB 13.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 346.3/385.0 MB 13.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 348.7/385.0 MB 13.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 351.0/385.0 MB 13.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 353.6/385.0 MB 13.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 356.0/385.0 MB 13.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 358.6/385.0 MB 13.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 361.2/385.0 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 362.5/385.0 MB 13.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 365.4/385.0 MB 13.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 367.8/385.0 MB 12.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 370.7/385.0 MB 12.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 373.3/385.0 MB 12.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.9/385.0 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  378.8/385.0 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.4/385.0 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.3/385.0 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.8/385.0 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.8/385.0 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.8/385.0 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 385.0/385.0 MB 12.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorflow-intel, tensorflow\n",
      "Successfully installed tensorflow-2.17.0 tensorflow-intel-2.17.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. [CNN assembling - MNIST](#CNN)\n",
    "   - 6.1 [A simple CNN architecture](#architecture)\n",
    "   - 6.2 [Preprocesing the data](#preprocesing)\n",
    "   - 6.3 [Building and training the CNN](#train)\n",
    "   - 6.4 [Testing the trained CNN](#testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.1 [A simple CNN architecture](#architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![er11](CNNARQUITECTURE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 [Preprocesing the data](#preprocesing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (8.1.7)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (1.26.4)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (4.25.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (5.9.5)\n",
      "Collecting pyarrow (from tensorflow-datasets)\n",
      "  Downloading pyarrow-17.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting simple-parsing (from tensorflow-datasets)\n",
      "  Downloading simple_parsing-0.1.6-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (2.4.0)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (4.66.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Collecting etils>=1.9.1 (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading etils-1.10.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2024.9.0)\n",
      "Collecting importlib_resources (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.12.2)\n",
      "Collecting zipp (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->tensorflow-datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing->tensorflow-datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading tensorflow_datasets-4.9.6-py3-none-any.whl (5.1 MB)\n",
      "   ---------------------------------------- 0.0/5.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/5.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.5/5.1 MB 1.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 0.8/5.1 MB 1.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.0/5.1 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 1.6/5.1 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 1.8/5.1 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.1/5.1 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 2.4/5.1 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 2.6/5.1 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.9/5.1 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.1/5.1 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 3.4/5.1 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 3.4/5.1 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 3.7/5.1 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 3.7/5.1 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 3.7/5.1 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 3.9/5.1 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 3.9/5.1 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 4.2/5.1 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.2/5.1 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.5/5.1 MB 990.6 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.5/5.1 MB 990.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 4.7/5.1 MB 970.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 4.7/5.1 MB 970.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.1/5.1 MB 941.6 kB/s eta 0:00:00\n",
      "Downloading etils-1.10.0-py3-none-any.whl (164 kB)\n",
      "Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl (101 kB)\n",
      "Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-17.0.0-cp311-cp311-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.2 MB 1.1 MB/s eta 0:00:22\n",
      "   - -------------------------------------- 0.8/25.2 MB 1.1 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.8/25.2 MB 1.1 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 1.0/25.2 MB 986.7 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 1.3/25.2 MB 945.5 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.6/25.2 MB 953.2 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 1.6/25.2 MB 953.2 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 1.8/25.2 MB 891.1 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 1.8/25.2 MB 891.1 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 2.1/25.2 MB 870.1 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 2.1/25.2 MB 870.1 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 2.1/25.2 MB 870.1 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 2.4/25.2 MB 794.4 kB/s eta 0:00:29\n",
      "   --- ------------------------------------ 2.4/25.2 MB 794.4 kB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 2.6/25.2 MB 774.3 kB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 2.9/25.2 MB 780.4 kB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 3.1/25.2 MB 795.5 kB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 3.4/25.2 MB 821.8 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 3.7/25.2 MB 845.4 kB/s eta 0:00:26\n",
      "   ------ --------------------------------- 3.9/25.2 MB 866.8 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 4.2/25.2 MB 892.5 kB/s eta 0:00:24\n",
      "   ------- -------------------------------- 4.5/25.2 MB 919.4 kB/s eta 0:00:23\n",
      "   ------- -------------------------------- 4.7/25.2 MB 938.2 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 5.0/25.2 MB 955.8 kB/s eta 0:00:22\n",
      "   -------- ------------------------------- 5.5/25.2 MB 989.8 kB/s eta 0:00:20\n",
      "   --------- ------------------------------ 5.8/25.2 MB 1.0 MB/s eta 0:00:20\n",
      "   --------- ------------------------------ 6.0/25.2 MB 1.0 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 6.3/25.2 MB 1.0 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 6.6/25.2 MB 1.1 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 6.8/25.2 MB 1.1 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 7.1/25.2 MB 1.1 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 7.3/25.2 MB 1.1 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.6/25.2 MB 1.1 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.9/25.2 MB 1.1 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 8.4/25.2 MB 1.1 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 8.7/25.2 MB 1.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 8.9/25.2 MB 1.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 9.2/25.2 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.4/25.2 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.7/25.2 MB 1.1 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 10.0/25.2 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 10.2/25.2 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 10.5/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 10.5/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 10.7/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 11.0/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 11.0/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 11.3/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.5/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.5/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.5/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.8/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.8/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 12.1/25.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 12.3/25.2 MB 1.0 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 12.3/25.2 MB 1.0 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 12.6/25.2 MB 1.0 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 12.6/25.2 MB 1.0 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 12.8/25.2 MB 1.0 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 13.1/25.2 MB 1.0 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 13.1/25.2 MB 1.0 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 13.4/25.2 MB 1.0 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 13.6/25.2 MB 1.0 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 13.9/25.2 MB 1.0 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 13.9/25.2 MB 1.0 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 14.2/25.2 MB 1.0 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 14.4/25.2 MB 1.0 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 14.7/25.2 MB 1.0 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 14.9/25.2 MB 1.0 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.2/25.2 MB 1.0 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.2/25.2 MB 1.0 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.5/25.2 MB 1.0 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 15.7/25.2 MB 1.0 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 16.0/25.2 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 16.0/25.2 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 16.3/25.2 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 16.3/25.2 MB 1.0 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 16.5/25.2 MB 1.0 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 16.8/25.2 MB 1.0 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 16.8/25.2 MB 1.0 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 17.0/25.2 MB 997.0 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 17.3/25.2 MB 997.7 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.6/25.2 MB 999.4 kB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 17.8/25.2 MB 1.0 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 18.1/25.2 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 18.4/25.2 MB 1.0 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.6/25.2 MB 1.0 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 18.9/25.2 MB 1.0 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 19.1/25.2 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.1/25.2 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.4/25.2 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 19.7/25.2 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 19.9/25.2 MB 1.0 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 20.2/25.2 MB 1.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.4/25.2 MB 1.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.7/25.2 MB 1.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 21.0/25.2 MB 1.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 21.0/25.2 MB 1.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 21.2/25.2 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.5/25.2 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.8/25.2 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 22.0/25.2 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 22.3/25.2 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.5/25.2 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.8/25.2 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 23.1/25.2 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.3/25.2 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.6/25.2 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.9/25.2 MB 1.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.1/25.2 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.2/25.2 MB 1.0 MB/s eta 0:00:00\n",
      "Downloading simple_parsing-0.1.6-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21545 sha256=02cc5e378df61a2255449ed4aa1fabd7894c147a04423c9c6555f17f307c54c3\n",
      "  Stored in directory: c:\\users\\usuario\\appdata\\local\\pip\\cache\\wheels\\90\\74\\b1\\9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, zipp, toml, pyarrow, promise, importlib_resources, immutabledict, googleapis-common-protos, etils, docstring-parser, tensorflow-metadata, simple-parsing, tensorflow-datasets\n",
      "Successfully installed dm-tree-0.1.8 docstring-parser-0.16 etils-1.10.0 googleapis-common-protos-1.65.0 immutabledict-4.2.0 importlib_resources-6.4.5 promise-2.3 pyarrow-17.0.0 simple-parsing-0.1.6 tensorflow-datasets-4.9.6 tensorflow-metadata-1.16.1 toml-0.10.2 zipp-3.20.2\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant packages\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![er1](MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset https://www.tensorflow.org/datasets/catalog/mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some constants/hyperparameters\n",
    "BUFFER_SIZE = 70_000 # for reshuffling\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\usuario\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9577ad5ce4140d881db59ae7a34a670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b617660a6de44ec0b00949c97f47d29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf98e956dc494f0eb32a5a8ab6f2d2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318f46ceab15476284affb63e64f6d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d610156bfbcb4866939df1c53b413ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10e71a771934d79b4921c2e511fcb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\usuario\\tensorflow_datasets\\mnist\\incomplete.NXFR7D_3.0.1\\mnist-train.tfrecord*...:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e032abed17426f8e35ed7429016f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ca57b2d34346499ea6e4e65fe32cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\usuario\\tensorflow_datasets\\mnist\\incomplete.NXFR7D_3.0.1\\mnist-test.tfrecord*...:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\usuario\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Downloading the MNIST dataset\n",
    "\n",
    "# When 'with_info' is set to True, tfds.load() returns two variables: \n",
    "# - the dataset (including the train and test sets) \n",
    "# - meta info regarding the dataset itself\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the train and test datasets\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to scale our image data (it is recommended to scale the pixel values in the range [0,1] )\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the validation set\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the test set\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffling the dataset\n",
    "train_and_validation_data = train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training + validation\n",
    "train_data = train_and_validation_data.skip(num_validation_samples)\n",
    "validation_data = train_and_validation_data.take(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching the data\n",
    "# NOTE: For proper functioning of the model, we need to create one big batch for the validation and test sets\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples) \n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the numpy arrays from the validation data for the calculation of the Confusion Matrix\n",
    "for images, labels in validation_data:\n",
    "    images_val = images.numpy()\n",
    "    labels_val = labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 [Building and training the CNN](#train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have preprocessed the dataset, we can define our CNN and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Outlining the model/architecture of our CNN\n",
    "# CONV -> MAXPOOL -> CONV -> MAXPOOL -> FLATTEN -> DENSE\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(50, 5, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
    "    # (2,2) is the default pool size so we could have just used MaxPooling2D() with no explicit arguments\n",
    "    tf.keras.layers.Conv2D(50, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10) # You can apply softmax activatio n here, see below for comentary\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                   </span>┃<span style=\"font-weight: bold\"> Output Shape            </span>┃<span style=\"font-weight: bold\">      Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,300</span> │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">22,550</span> │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>)            │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">12,510</span> │\n",
       "└────────────────────────────────┴─────────────────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m50\u001b[0m)      │        \u001b[38;5;34m1,300\u001b[0m │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m50\u001b[0m)      │            \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m50\u001b[0m)      │       \u001b[38;5;34m22,550\u001b[0m │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │            \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m)            │            \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)              │       \u001b[38;5;34m12,510\u001b[0m │\n",
       "└────────────────────────────────┴─────────────────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,360</span> (142.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m36,360\u001b[0m (142.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,360</span> (142.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m36,360\u001b[0m (142.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A brief summary of the model and parameters\n",
    "model.summary(line_length = 75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function\n",
    "\n",
    "# In general, our model needs to output probabilities of each class, \n",
    "# which can be achieved with a softmax activation in the last dense layer\n",
    "\n",
    "# However, when using the softmax activation, the loss can rarely be unstable\n",
    "\n",
    "# Thus, instead of incorporating the softmax into the model itself,\n",
    "# we use a loss calculation that automatically corrects for the missing softmax\n",
    "\n",
    "# That is the reason for 'from_logits=True'\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model with Adam optimizer and the cathegorical crossentropy as a loss function\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'auto',    \n",
    "    min_delta = 0,\n",
    "    patience = 2,\n",
    "    verbose = 0, \n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 - 5s - 11ms/step - accuracy: 0.9942 - loss: 0.0186 - val_accuracy: 0.9935 - val_loss: 0.0166\n",
      "Epoch 2/20\n",
      "422/422 - 4s - 9ms/step - accuracy: 0.9954 - loss: 0.0153 - val_accuracy: 0.9945 - val_loss: 0.0167\n",
      "Epoch 3/20\n",
      "422/422 - 4s - 9ms/step - accuracy: 0.9953 - loss: 0.0141 - val_accuracy: 0.9930 - val_loss: 0.0170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c76e5f5090>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the network\n",
    "model.fit(\n",
    "    train_data, \n",
    "    epochs = NUM_EPOCHS, \n",
    "    callbacks = [early_stopping], \n",
    "    validation_data = validation_data,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.9874 - loss: 0.0405\n"
     ]
    }
   ],
   "source": [
    "# Testing our model\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0405. Test accuracy: 98.74%\n"
     ]
    }
   ],
   "source": [
    "# Printing the test results\n",
    "print('Test loss: {0:.4f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting images and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test_data into 2 arrays, containing the images and the corresponding labels\n",
    "for images, labels in test_data.take(1):\n",
    "    images_test = images.numpy()\n",
    "    labels_test = labels.numpy()\n",
    "\n",
    "# Reshape the images into 28x28 form, suitable for matplotlib (original dimensions: 28x28x1)\n",
    "images_plot = np.reshape(images_test, (10000,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAACuCAYAAABAzl3QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFG0lEQVR4nO3dzyt0bRzH8TP3UCg/mpUFVlaSkqSIFBaSnSj+AT9q8i8oVrKSZG+jYSNZKFESs5CywkoWFsiPUsIwz+Kpp56u7+jyY9w+Z96v5bfLmWvq3ek+58zcE0mn0+kAEPTnb28A+CzihSzihSzihSzihSzihSzihSzihSzihaw834WRSCSb+wD+4/vQlzMvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZHn/9jDCpbCw0Jx3dHR4H+Px8dGZbW5ufnpPH8WZF7KIF7KIF7KIF7Jy7oItGo06s8nJSXPtwMCA93GtC5VMxz0/Pzfn6XTa+/Ws92HNgiAIJiYmnNng4KC5tqKiwnsPqVTKme3t7Zlr+/v7ndnl5aX3a1k480IW8UIW8UIW8UIW8UJWJO15iRuJRLK9l29VXFxszhOJhDPr6urK9nb+Jx6Pm/Pj42Nn1tfXZ65tbGx0ZvX19V/b2A/LdHfE964LZ17IIl7IIl7IIl7ICsUFW2VlpTNbW1sz19bW1nof9+LiwpmNjIyYa5uampzZ8PCwuTYWi3nv4aclk0lnNjMzY669ubn50mttbW2Zcy7YEHrEC1nEC1nEC1nEC1mhuNuwsbHhzD7yLVjrrkIQBEFPT48zOzo68j7uzs6OOW9ubvY+xkc8PT2Z88XFRWc2NTVlrrU+IG59SzibuNuA0CNeyCJeyCJeyArFt4c7OzudWaZ/9L++vnr9fRAEwcnJydc29g1ub2/N+dLSkjObnp42156dnX3nln4NzryQRbyQRbyQRbyQRbyQFYrHw9ZbeHt7M9e+vLw4s4KCgm/fUxAEQV1dnTkfHx835/f3985sfn7eXHt6evrpff12PB5G6BEvZBEvZBEvZIXigm12dtaZjY6Oev/92NiYObc+B/vw8OC/MXwKF2wIPeKFLOKFLOKFLOKFrFDcbSgpKXFmy8vL5tqPfKt4e3vbmXV3d5trn5+fvY+L93G3AaFHvJBFvJBFvJAVigs2S1lZmTlfWVlxZu3t7d7H3d/fN+ctLS3ex8D7uGBD6BEvZBEvZBEvZBEvZIX2bkMmRUVFzmx9fd1c29ra6n3chYUFZ5bpW8I8Sn4fdxsQesQLWcQLWcQLWTl3wWYpLS0154lEwpl95PPAbW1t5nx3d9f7GLmICzaEHvFCFvFCFvFCFvFCFncb3tHQ0ODMrG8UB4H92Hlubs5cG4/Hv7SvsONuA0KPeCGLeCGLeCErFL89nC0HBwfO7O7uzlxrXbBVVVWZa6PRqDm3fhcZmXHmhSzihSzihSzihSzihaycu9uQl+e+5Uy/75tMJp1ZLBbzfq3e3l5znukYV1dX3scGZ14II17IIl7IIl7IyrnP81oXbIeHh+bampqarOyhvLzcnHPB9i8+z4vQI17IIl7IIl7IIl7IyrnHw6lUypll+v/HhoaGnFl+fr65trq62pmtrq6aa6+vr9/bIjxx5oUs4oUs4oUs4oWsnHs8jN+Px8MIPeKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLO9vD/t+QBj4KZx5IYt4IYt4IYt4IYt4IYt4IYt4IYt4IYt4Iesf6XIjavaLCioAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "# The image to be displayed and tested\n",
    "i = 502\n",
    "\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.axis('off')\n",
    "plt.imshow(images_plot[i-1], cmap=\"gray\", aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Print the correct label for the image\n",
    "print(\"Label: {}\".format(labels_test[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9oAAAGsCAYAAAAi89+yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAh8klEQVR4nO3df3TV9WH/8VcAgQxJEDoSUgHROfH3D1Aacd2mOTLnPHLKsdpDz6HqdKcLVmTVwVb8saqoXS1DEdTj0LZStd3Aao86hh3OFRGx9GhrUVerTJu4HiVROqIl9/tHz3K+qZ616jtcEh+Pc+45zed+cnnxKdU+uT9SU6lUKgEAAACKGFTtAQAAADCQCG0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQ0pNoD3o/u7u688sorGTlyZGpqaqo9BwAAgAGuUqnkjTfeSFNTUwYN+r+fs+6Xof3KK69k/Pjx1Z4BAADAh8y2bduy7777/p/n9MvQHjlyZJJf/Qbr6uqqvAYAAICBrrOzM+PHj+/p0f9Lvwzt/325eF1dndAGAABgt/lt3r7sw9AAAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgoPcc2o888khOO+20NDU1paamJmvWrOl1f6VSyaWXXppx48altrY2LS0tee6553qd89prr2X27Nmpq6vLqFGjcu655+bNN9/8QL8RAAAA2BO859DesWNHjjzyyCxbtuxd77/uuuuydOnSrFixIhs3bsyIESMyY8aM7Ny5s+ec2bNn54c//GHWrl2b+++/P4888kjOP//89/+7AAAAgD1ETaVSqbzvb66pyerVqzNz5swkv3o2u6mpKX/1V3+Vz3/+80mSjo6ONDQ05Pbbb89ZZ52VZ555Joccckg2bdqUqVOnJkkefPDB/Omf/mn+67/+K01NTb/x1+3s7Ex9fX06OjpSV1f3fucDAADAb+W9dGjR92i/8MILaWtrS0tLS8+x+vr6TJs2LRs2bEiSbNiwIaNGjeqJ7CRpaWnJoEGDsnHjxnd93K6urnR2dva6AQAAwJ6oaGi3tbUlSRoaGnodb2ho6Lmvra0tY8eO7XX/kCFDMnr06J5zft3ixYtTX1/fcxs/fnzJ2QAAAFBMv/jU8YULF6ajo6Pntm3btmpPAgAAgHdVNLQbGxuTJO3t7b2Ot7e399zX2NiYV199tdf9v/zlL/Paa6/1nPPrhg0blrq6ul43AAAA2BMNKflgkyZNSmNjY9atW5ejjjoqya/eML5x48Z89rOfTZI0Nzdn+/bt2bx5c6ZMmZIkefjhh9Pd3Z1p06aVnAMAAHwA+y34TrUn7HF+es2p1Z5AP/CeQ/vNN9/M888/3/P1Cy+8kC1btmT06NGZMGFC5s2blyuvvDIHHnhgJk2alEWLFqWpqannk8kPPvjg/Mmf/EnOO++8rFixIm+//Xbmzp2bs84667f6xHEAAADYk73n0H7iiSfyx3/8xz1fz58/P0kyZ86c3H777bnkkkuyY8eOnH/++dm+fXtOOOGEPPjggxk+fHjP99x5552ZO3duTjrppAwaNCizZs3K0qVLC/x2AAAAoLo+0M/RrhY/RxsAAPqel46/k5eOf3hV7edoAwAAwIed0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEHFQ3vXrl1ZtGhRJk2alNra2hxwwAH54he/mEql0nNOpVLJpZdemnHjxqW2tjYtLS157rnnSk8BAACA3a54aF977bVZvnx5brzxxjzzzDO59tprc9111+WGG27oOee6667L0qVLs2LFimzcuDEjRozIjBkzsnPnztJzAAAAYLcaUvoBv/e97+X000/PqaeemiTZb7/98o1vfCOPP/54kl89m71kyZJ84QtfyOmnn54k+epXv5qGhoasWbMmZ511VulJAAAAsNsUf0b7+OOPz7p16/Lss88mSX7wgx/k0UcfzSmnnJIkeeGFF9LW1paWlpae76mvr8+0adOyYcOGd33Mrq6udHZ29roBAADAnqj4M9oLFixIZ2dnJk+enMGDB2fXrl256qqrMnv27CRJW1tbkqShoaHX9zU0NPTc9+sWL16cK664ovRUAAAAKK74M9r33HNP7rzzzqxatSpPPvlk7rjjjvz93/997rjjjvf9mAsXLkxHR0fPbdu2bQUXAwAAQDnFn9G++OKLs2DBgp73Wh9++OF58cUXs3jx4syZMyeNjY1Jkvb29owbN67n+9rb23PUUUe962MOGzYsw4YNKz0VAAAAiiv+jPYvfvGLDBrU+2EHDx6c7u7uJMmkSZPS2NiYdevW9dzf2dmZjRs3prm5ufQcAAAA2K2KP6N92mmn5aqrrsqECRNy6KGH5vvf/36uv/76nHPOOUmSmpqazJs3L1deeWUOPPDATJo0KYsWLUpTU1NmzpxZeg4AAADsVsVD+4YbbsiiRYvyl3/5l3n11VfT1NSUv/iLv8ill17ac84ll1ySHTt25Pzzz8/27dtzwgkn5MEHH8zw4cNLzwEAAIDdqqZSqVSqPeK96uzsTH19fTo6OlJXV1ftOQAAMCDtt+A71Z6wx/npNadWewJV8l46tPh7tAEAAODDTGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKCgPgntl19+OZ/+9KczZsyY1NbW5vDDD88TTzzRc3+lUsmll16acePGpba2Ni0tLXnuuef6YgoAAADsVsVD+/XXX8/06dOz11575YEHHsiPfvSjfPnLX84+++zTc851112XpUuXZsWKFdm4cWNGjBiRGTNmZOfOnaXnAAAAwG41pPQDXnvttRk/fnxWrlzZc2zSpEk9/7lSqWTJkiX5whe+kNNPPz1J8tWvfjUNDQ1Zs2ZNzjrrrNKTAAAAYLcp/oz2t7/97UydOjVnnHFGxo4dm6OPPjq33nprz/0vvPBC2tra0tLS0nOsvr4+06ZNy4YNG971Mbu6utLZ2dnrBgAAAHui4qH9k5/8JMuXL8+BBx6Yhx56KJ/97Gfzuc99LnfccUeSpK2tLUnS0NDQ6/saGhp67vt1ixcvTn19fc9t/PjxpWcDAABAEcVDu7u7O8ccc0yuvvrqHH300Tn//PNz3nnnZcWKFe/7MRcuXJiOjo6e27Zt2wouBgAAgHKKh/a4ceNyyCGH9Dp28MEH56WXXkqSNDY2Jkna29t7ndPe3t5z368bNmxY6urqet0AAABgT1Q8tKdPn56tW7f2Ovbss89m4sSJSX71wWiNjY1Zt25dz/2dnZ3ZuHFjmpubS88BAACA3ar4p45fdNFFOf7443P11Vfnk5/8ZB5//PHccsstueWWW5IkNTU1mTdvXq688soceOCBmTRpUhYtWpSmpqbMnDmz9BwAAADYrYqH9rHHHpvVq1dn4cKF+bu/+7tMmjQpS5YsyezZs3vOueSSS7Jjx46cf/752b59e0444YQ8+OCDGT58eOk5AAAAsFvVVCqVSrVHvFednZ2pr69PR0eH92sDAEAf2W/Bd6o9YY/z02tOrfYEquS9dGjx92gDAADAh5nQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQX0e2tdcc01qamoyb968nmM7d+5Ma2trxowZk7333juzZs1Ke3t7X08BAACAPtenob1p06bcfPPNOeKII3odv+iii3Lfffflm9/8ZtavX59XXnkln/jEJ/pyCgAAAOwWfRbab775ZmbPnp1bb701++yzT8/xjo6O3Hbbbbn++utz4oknZsqUKVm5cmW+973v5bHHHuurOQAAALBb9Flot7a25tRTT01LS0uv45s3b87bb7/d6/jkyZMzYcKEbNiw4V0fq6urK52dnb1uAAAAsCca0hcPetddd+XJJ5/Mpk2b3nFfW1tbhg4dmlGjRvU63tDQkLa2tnd9vMWLF+eKK67oi6kAAABQVPFntLdt25YLL7wwd955Z4YPH17kMRcuXJiOjo6e27Zt24o8LgAAAJRWPLQ3b96cV199Ncccc0yGDBmSIUOGZP369Vm6dGmGDBmShoaGvPXWW9m+fXuv72tvb09jY+O7PuawYcNSV1fX6wYAAAB7ouIvHT/ppJPy1FNP9Tp29tlnZ/Lkyfnrv/7rjB8/PnvttVfWrVuXWbNmJUm2bt2al156Kc3NzaXnAAAAwG5VPLRHjhyZww47rNexESNGZMyYMT3Hzz333MyfPz+jR49OXV1dLrjggjQ3N+djH/tY6TkAAACwW/XJh6H9Jl/5ylcyaNCgzJo1K11dXZkxY0ZuuummakwBAACAomoqlUql2iPeq87OztTX16ejo8P7tQEAoI/st+A71Z6wx/npNadWewJV8l46tM9+jjYAAAB8GAltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUJLQBAACgIKENAAAABQltAAAAKEhoAwAAQEFCGwAAAAoS2gAAAFCQ0AYAAICChDYAAAAUVDy0Fy9enGOPPTYjR47M2LFjM3PmzGzdurXXOTt37kxra2vGjBmTvffeO7NmzUp7e3vpKQAAALDbFQ/t9evXp7W1NY899ljWrl2bt99+OyeffHJ27NjRc85FF12U++67L9/85jezfv36vPLKK/nEJz5RegoAAADsdkNKP+CDDz7Y6+vbb789Y8eOzebNm/Pxj388HR0due2227Jq1aqceOKJSZKVK1fm4IMPzmOPPZaPfexj73jMrq6udHV19Xzd2dlZejYAAAAU0efv0e7o6EiSjB49OkmyefPmvP3222lpaek5Z/LkyZkwYUI2bNjwro+xePHi1NfX99zGjx/f17MBAADgfenT0O7u7s68efMyffr0HHbYYUmStra2DB06NKNGjep1bkNDQ9ra2t71cRYuXJiOjo6e27Zt2/pyNgAAALxvxV86/v9rbW3N008/nUcfffQDPc6wYcMybNiwQqsAAACg7/TZM9pz587N/fffn+9+97vZd999e443Njbmrbfeyvbt23ud397ensbGxr6aAwAAALtF8dCuVCqZO3duVq9enYcffjiTJk3qdf+UKVOy1157Zd26dT3Htm7dmpdeeinNzc2l5wAAAMBuVfyl462trVm1alXuvffejBw5sud91/X19amtrU19fX3OPffczJ8/P6NHj05dXV0uuOCCNDc3v+snjgMAAEB/Ujy0ly9fniT5oz/6o17HV65cmc985jNJkq985SsZNGhQZs2ala6ursyYMSM33XRT6SkAAACw2xUP7Uql8hvPGT58eJYtW5Zly5aV/uUBAACgqvr852gDAADAh4nQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQUIbAAAAChLaAAAAUJDQBgAAgIKENgAAABQktAEAAKAgoQ0AAAAFCW0AAAAoSGgDAABAQVUN7WXLlmW//fbL8OHDM23atDz++OPVnAMAAAAf2JBq/cJ333135s+fnxUrVmTatGlZsmRJZsyYka1bt2bs2LHVmgXQJ/Zb8J1qT9jj/PSaU6s9AQCgT1QttK+//vqcd955Ofvss5MkK1asyHe+85384z/+YxYsWNDr3K6urnR1dfV83dHRkSTp7OzcfYMBPoDurl9Ue8Iexz/DAfZ8/v31Tv799eH1v//dVyqV33huTeW3Oauwt956K7/zO7+Tb33rW5k5c2bP8Tlz5mT79u259957e51/+eWX54orrtjNKwEAAKC3bdu2Zd999/0/z6nKM9o///nPs2vXrjQ0NPQ63tDQkB//+MfvOH/hwoWZP39+z9fd3d157bXXMmbMmNTU1PT53oGgs7Mz48ePz7Zt21JXV1ftOQOG69o3XNe+49r2Dde1b7iufce17Ruua99wXfuG6/reVSqVvPHGG2lqavqN51btpePvxbBhwzJs2LBex0aNGlWdMf1cXV2d/yH1Ade1b7iufce17Ruua99wXfuOa9s3XNe+4br2Ddf1vamvr/+tzqvKp45/5CMfyeDBg9Pe3t7reHt7exobG6sxCQAAAIqoSmgPHTo0U6ZMybp163qOdXd3Z926dWlubq7GJAAAACiiai8dnz9/fubMmZOpU6fmuOOOy5IlS7Jjx46eTyGnrGHDhuWyyy57x0vw+WBc177huvYd17ZvuK59w3XtO65t33Bd+4br2jdc175VlU8d/1833nhjvvSlL6WtrS1HHXVUli5dmmnTplVrDgAAAHxgVQ1tAAAAGGiq8h5tAAAAGKiENgAAABQktAEAAKAgoQ0AAAAFCe0PgWXLlmW//fbL8OHDM23atDz++OPVntTvPfLIIznttNPS1NSUmpqarFmzptqTBoTFixfn2GOPzciRIzN27NjMnDkzW7durfasfm/58uU54ogjUldXl7q6ujQ3N+eBBx6o9qwB55prrklNTU3mzZtX7Sn93uWXX56amppet8mTJ1d71oDw8ssv59Of/nTGjBmT2traHH744XniiSeqPavf22+//d7xZ7ampiatra3Vntav7dq1K4sWLcqkSZNSW1ubAw44IF/84hfjs5w/uDfeeCPz5s3LxIkTU1tbm+OPPz6bNm2q9qwBRWgPcHfffXfmz5+fyy67LE8++WSOPPLIzJgxI6+++mq1p/VrO3bsyJFHHplly5ZVe8qAsn79+rS2tuaxxx7L2rVr8/bbb+fkk0/Ojh07qj2tX9t3331zzTXXZPPmzXniiSdy4okn5vTTT88Pf/jDak8bMDZt2pSbb745RxxxRLWnDBiHHnpofvazn/XcHn300WpP6vdef/31TJ8+PXvttVceeOCB/OhHP8qXv/zl7LPPPtWe1u9t2rSp15/XtWvXJknOOOOMKi/r36699tosX748N954Y5555plce+21ue6663LDDTdUe1q/9+d//udZu3Ztvva1r+Wpp57KySefnJaWlrz88svVnjZg+PFeA9y0adNy7LHH5sYbb0ySdHd3Z/z48bnggguyYMGCKq8bGGpqarJ69erMnDmz2lMGnP/+7//O2LFjs379+nz84x+v9pwBZfTo0fnSl76Uc889t9pT+r0333wzxxxzTG666aZceeWVOeqoo7JkyZJqz+rXLr/88qxZsyZbtmyp9pQBZcGCBfmP//iP/Pu//3u1pwx48+bNy/3335/nnnsuNTU11Z7Tb/3Zn/1ZGhoactttt/UcmzVrVmpra/P1r3+9isv6t//5n//JyJEjc++99+bUU0/tOT5lypSccsopufLKK6u4buDwjPYA9tZbb2Xz5s1paWnpOTZo0KC0tLRkw4YNVVwGv52Ojo4kv4pCyti1a1fuuuuu7NixI83NzdWeMyC0trbm1FNP7fXPWj645557Lk1NTdl///0ze/bsvPTSS9We1O99+9vfztSpU3PGGWdk7NixOfroo3PrrbdWe9aA89Zbb+XrX/96zjnnHJH9AR1//PFZt25dnn322STJD37wgzz66KM55ZRTqrysf/vlL3+ZXbt2Zfjw4b2O19bWevVQQUOqPYC+8/Of/zy7du1KQ0NDr+MNDQ358Y9/XKVV8Nvp7u7OvHnzMn369Bx22GHVntPvPfXUU2lubs7OnTuz9957Z/Xq1TnkkEOqPavfu+uuu/Lkk096X1th06ZNy+23356DDjooP/vZz3LFFVfkD/7gD/L0009n5MiR1Z7Xb/3kJz/J8uXLM3/+/PzN3/xNNm3alM997nMZOnRo5syZU+15A8aaNWuyffv2fOYzn6n2lH5vwYIF6ezszOTJkzN48ODs2rUrV111VWbPnl3taf3ayJEj09zcnC9+8Ys5+OCD09DQkG984xvZsGFDfu/3fq/a8wYMoQ3skVpbW/P000/7m9VCDjrooGzZsiUdHR351re+lTlz5mT9+vVi+wPYtm1bLrzwwqxdu/Ydzwrwwfz/z1YdccQRmTZtWiZOnJh77rnH2x0+gO7u7kydOjVXX311kuToo4/O008/nRUrVgjtgm677baccsopaWpqqvaUfu+ee+7JnXfemVWrVuXQQw/Nli1bMm/evDQ1Nfkz+wF97WtfyznnnJOPfvSjGTx4cI455ph86lOfyubNm6s9bcAQ2gPYRz7ykQwePDjt7e29jre3t6exsbFKq+A3mzt3bu6///488sgj2Xfffas9Z0AYOnRoz99ST5kyJZs2bco//MM/5Oabb67ysv5r8+bNefXVV3PMMcf0HNu1a1ceeeSR3Hjjjenq6srgwYOruHDgGDVqVH7/938/zz//fLWn9Gvjxo17x1+uHXzwwfmnf/qnKi0aeF588cX867/+a/75n/+52lMGhIsvvjgLFizIWWedlSQ5/PDD8+KLL2bx4sVC+wM64IADsn79+uzYsSOdnZ0ZN25czjzzzOy///7VnjZgeI/2ADZ06NBMmTIl69at6znW3d2ddevWeW8me6RKpZK5c+dm9erVefjhhzNp0qRqTxqwuru709XVVe0Z/dpJJ52Up556Klu2bOm5TZ06NbNnz86WLVtEdkFvvvlm/vM//zPjxo2r9pR+bfr06e/4kYnPPvtsJk6cWKVFA8/KlSszduzYXh8wxfv3i1/8IoMG9c6VwYMHp7u7u0qLBp4RI0Zk3Lhxef311/PQQw/l9NNPr/akAcMz2gPc/PnzM2fOnEydOjXHHXdclixZkh07duTss8+u9rR+7c033+z1zMoLL7yQLVu2ZPTo0ZkwYUIVl/Vvra2tWbVqVe69996MHDkybW1tSZL6+vrU1tZWeV3/tXDhwpxyyimZMGFC3njjjaxatSr/9m//loceeqja0/q1kSNHvuPzA0aMGJExY8b4XIEP6POf/3xOO+20TJw4Ma+88kouu+yyDB48OJ/61KeqPa1fu+iii3L88cfn6quvzic/+ck8/vjjueWWW3LLLbdUe9qA0N3dnZUrV2bOnDkZMsT/xS7htNNOy1VXXZUJEybk0EMPzfe///1cf/31Oeecc6o9rd976KGHUqlUctBBB+X555/PxRdfnMmTJ2uEkioMeDfccENlwoQJlaFDh1aOO+64ymOPPVbtSf3ed7/73UqSd9zmzJlT7Wn92rtd0ySVlStXVntav3bOOedUJk6cWBk6dGjld3/3dysnnXRS5V/+5V+qPWtA+sM//MPKhRdeWO0Z/d6ZZ55ZGTduXGXo0KGVj370o5Uzzzyz8vzzz1d71oBw3333VQ477LDKsGHDKpMnT67ccsst1Z40YDz00EOVJJWtW7dWe8qA0dnZWbnwwgsrEyZMqAwfPryy//77V/72b/+20tXVVe1p/d7dd99d2X///StDhw6tNDY2VlpbWyvbt2+v9qwBxc/RBgAAgIK8RxsAAAAKEtoAAABQkNAGAACAgoQ2AAAAFCS0AQAAoCChDQAAAAUJbQAAAChIaAMAAEBBQhsAAAAKEtoAAABQkNAGAACAgv4f5DuYm5jVeOgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the model's predictions (logits)\n",
    "predictions = model.predict(images_test[i-1:i])\n",
    "\n",
    "# Convert those predictions into probabilities (recall that we incorporated the softmaxt activation into the loss function)\n",
    "probabilities = tf.nn.softmax(predictions).numpy()\n",
    "# Convert the probabilities into percentages\n",
    "probabilities = probabilities*100\n",
    "\n",
    "\n",
    "# Create a bar chart to plot the probabilities for each class\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(x=[1,2,3,4,5,6,7,8,9,10], height=probabilities[0], tick_label=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. [Tensorboard: Visualization  tool for TensorFlow](#tensorflow)\n",
    "   - 7.1 [Tensorboard on the MINST example](#example)\n",
    "   - 7.2 [Visualization Confusion Matriz with Tensorboard](#confusion)\n",
    "   - 7.3 [Using tensorboard to tune hyperparameters](#hyperparameter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 [Tensorboard on the MINST example](#example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant packages\n",
    "import io\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Outlining the model/architecture of our CNN\n",
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(50, 5, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Conv2D(50, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                   </span>┃<span style=\"font-weight: bold\"> Output Shape            </span>┃<span style=\"font-weight: bold\">      Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,300</span> │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">22,550</span> │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>)            │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">12,510</span> │\n",
       "└────────────────────────────────┴─────────────────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m50\u001b[0m)      │        \u001b[38;5;34m1,300\u001b[0m │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m50\u001b[0m)      │            \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m50\u001b[0m)      │       \u001b[38;5;34m22,550\u001b[0m │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │            \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m)            │            \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────┼─────────────────────────┼──────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)              │       \u001b[38;5;34m12,510\u001b[0m │\n",
       "└────────────────────────────────┴─────────────────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,360</span> (142.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m36,360\u001b[0m (142.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,360</span> (142.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m36,360\u001b[0m (142.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A brief summary of the model and parameters\n",
    "model1.summary(line_length = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model with Adam optimizer and the cathegorical crossentropy as a loss function\n",
    "model1.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"Logs\\\\fit\\\\\" + \"run-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "    \n",
    "    # Save the plot to a PNG in memory.\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside the notebook.\n",
    "    plt.close(figure)\n",
    "    \n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a file writer variable for logging purposes\n",
    "file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')\n",
    "\n",
    "def log_confusion_matrix(epoch, logs):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "    test_pred_raw = model.predict(images_val)\n",
    "    test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "    # Calculate the confusion matrix.\n",
    "    cm = sklearn.metrics.confusion_matrix(labels_val, test_pred)\n",
    "    \n",
    "    # Log the confusion matrix as an image summary.\n",
    "    figure = plot_confusion_matrix(cm, class_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the callbacks\n",
    "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'auto',\n",
    "    min_delta = 0,\n",
    "    patience = 2,\n",
    "    verbose = 0, \n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "422/422 - 6s - 14ms/step - accuracy: 0.9247 - loss: 0.2728 - val_accuracy: 0.9768 - val_loss: 0.0782\n",
      "Epoch 2/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9785 - loss: 0.0701 - val_accuracy: 0.9865 - val_loss: 0.0460\n",
      "Epoch 3/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9841 - loss: 0.0523 - val_accuracy: 0.9863 - val_loss: 0.0442\n",
      "Epoch 4/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.9869 - loss: 0.0431 - val_accuracy: 0.9902 - val_loss: 0.0363\n",
      "Epoch 5/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9888 - loss: 0.0364 - val_accuracy: 0.9918 - val_loss: 0.0302\n",
      "Epoch 6/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9904 - loss: 0.0305 - val_accuracy: 0.9932 - val_loss: 0.0230\n",
      "Epoch 7/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9910 - loss: 0.0280 - val_accuracy: 0.9928 - val_loss: 0.0230\n",
      "Epoch 8/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9929 - loss: 0.0234 - val_accuracy: 0.9948 - val_loss: 0.0150\n",
      "Epoch 9/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9937 - loss: 0.0204 - val_accuracy: 0.9960 - val_loss: 0.0135\n",
      "Epoch 10/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9935 - loss: 0.0198 - val_accuracy: 0.9950 - val_loss: 0.0155\n",
      "Epoch 11/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.9942 - loss: 0.0176 - val_accuracy: 0.9955 - val_loss: 0.0124\n",
      "Epoch 12/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.9956 - loss: 0.0140 - val_accuracy: 0.9973 - val_loss: 0.0106\n",
      "Epoch 13/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 6s - 13ms/step - accuracy: 0.9958 - loss: 0.0130 - val_accuracy: 0.9970 - val_loss: 0.0103\n",
      "Epoch 14/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.9962 - loss: 0.0114 - val_accuracy: 0.9970 - val_loss: 0.0092\n",
      "Epoch 15/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.9964 - loss: 0.0108 - val_accuracy: 0.9968 - val_loss: 0.0079\n",
      "Epoch 16/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 6s - 14ms/step - accuracy: 0.9976 - loss: 0.0084 - val_accuracy: 0.9975 - val_loss: 0.0107\n",
      "Epoch 17/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.9973 - loss: 0.0083 - val_accuracy: 0.9980 - val_loss: 0.0056\n",
      "Epoch 18/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9968 - loss: 0.0094 - val_accuracy: 0.9983 - val_loss: 0.0062\n",
      "Epoch 19/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.9980 - loss: 0.0068 - val_accuracy: 0.9985 - val_loss: 0.0057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c84b95b0d0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the network\n",
    "model1.fit(\n",
    "    train_data, \n",
    "    epochs = NUM_EPOCHS, \n",
    "    callbacks = [tensorboard_callback, cm_callback, early_stopping], \n",
    "    validation_data = validation_data,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.9874 - loss: 0.0405\n"
     ]
    }
   ],
   "source": [
    "# Testing our model\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0405. Test accuracy: 98.74%\n"
     ]
    }
   ],
   "source": [
    "# Printing the test results\n",
    "print('Test loss: {0:.4f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 [Visualization Confusion Matriz with Tensorboard](#confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 21728), started 0:11:13 ago. (Use '!kill 21728' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ee4dfa143b0b1727\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ee4dfa143b0b1727\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the Tensorboard extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs/fit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 [Using tensorboard to tune hyperparameters](#hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant packages\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hypermatarest we would test and their range\n",
    "HP_FILTER_SIZE = hp.HParam('filter_size', hp.Discrete([3,5,7]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "# Logging setup info\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_FILTER_SIZE, HP_OPTIMIZER],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cerating functions for training our model and for logging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping our model and training in a function\n",
    "def train_test_model(hparams):\n",
    "    \n",
    "    # Outlining the model/architecture of our CNN\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(50, hparams[HP_FILTER_SIZE], activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(50, hparams[HP_FILTER_SIZE], activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    \n",
    "    # Defining the loss function\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compiling the model with parameter value for the optimizer\n",
    "    model.compile(optimizer=hparams[HP_OPTIMIZER], loss=loss_fn, metrics=['accuracy'])\n",
    "    \n",
    "    # Defining early stopping to prevent overfitting\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        mode = 'auto',\n",
    "        min_delta = 0,\n",
    "        patience = 2,\n",
    "        verbose = 0, \n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    \n",
    "    # Training the model\n",
    "    model.fit(\n",
    "        train_data, \n",
    "        epochs = NUM_EPOCHS,\n",
    "        callbacks = [early_stopping],\n",
    "        validation_data = validation_data,\n",
    "        verbose = 2\n",
    "    )\n",
    "    \n",
    "    _, accuracy = model.evaluate(test_data)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to log the resuls\n",
    "def run(log_dir, hparams):\n",
    "    \n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with the different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'filter_size': 3, 'optimizer': 'adam'}\n",
      "Epoch 1/20\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9134 - loss: 0.2955 - val_accuracy: 0.9702 - val_loss: 0.0958\n",
      "Epoch 2/20\n",
      "422/422 - 6s - 15ms/step - accuracy: 0.9770 - loss: 0.0765 - val_accuracy: 0.9768 - val_loss: 0.0658\n",
      "Epoch 3/20\n",
      "422/422 - 12s - 29ms/step - accuracy: 0.9823 - loss: 0.0580 - val_accuracy: 0.9850 - val_loss: 0.0471\n",
      "Epoch 4/20\n",
      "422/422 - 12s - 28ms/step - accuracy: 0.9853 - loss: 0.0470 - val_accuracy: 0.9860 - val_loss: 0.0443\n",
      "Epoch 5/20\n",
      "422/422 - 11s - 27ms/step - accuracy: 0.9870 - loss: 0.0413 - val_accuracy: 0.9898 - val_loss: 0.0347\n",
      "Epoch 6/20\n",
      "422/422 - 10s - 23ms/step - accuracy: 0.9886 - loss: 0.0349 - val_accuracy: 0.9912 - val_loss: 0.0328\n",
      "Epoch 7/20\n",
      "422/422 - 4s - 10ms/step - accuracy: 0.9906 - loss: 0.0308 - val_accuracy: 0.9897 - val_loss: 0.0299\n",
      "Epoch 8/20\n",
      "422/422 - 4s - 11ms/step - accuracy: 0.9923 - loss: 0.0262 - val_accuracy: 0.9932 - val_loss: 0.0197\n",
      "Epoch 9/20\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9922 - loss: 0.0247 - val_accuracy: 0.9953 - val_loss: 0.0149\n",
      "Epoch 10/20\n",
      "422/422 - 10s - 23ms/step - accuracy: 0.9934 - loss: 0.0215 - val_accuracy: 0.9940 - val_loss: 0.0171\n",
      "Epoch 11/20\n",
      "422/422 - 12s - 29ms/step - accuracy: 0.9938 - loss: 0.0193 - val_accuracy: 0.9945 - val_loss: 0.0175\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 776ms/step - accuracy: 0.9907 - loss: 0.0294\n",
      "--- Starting trial: run-1\n",
      "{'filter_size': 3, 'optimizer': 'sgd'}\n",
      "Epoch 1/20\n",
      "422/422 - 13s - 32ms/step - accuracy: 0.6219 - loss: 1.4350 - val_accuracy: 0.8432 - val_loss: 0.5405\n",
      "Epoch 2/20\n",
      "422/422 - 12s - 29ms/step - accuracy: 0.8852 - loss: 0.3975 - val_accuracy: 0.8952 - val_loss: 0.3311\n",
      "Epoch 3/20\n",
      "422/422 - 12s - 29ms/step - accuracy: 0.9124 - loss: 0.2992 - val_accuracy: 0.9190 - val_loss: 0.2674\n",
      "Epoch 4/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9276 - loss: 0.2469 - val_accuracy: 0.9285 - val_loss: 0.2448\n",
      "Epoch 5/20\n",
      "422/422 - 12s - 28ms/step - accuracy: 0.9385 - loss: 0.2122 - val_accuracy: 0.9400 - val_loss: 0.2028\n",
      "Epoch 6/20\n",
      "422/422 - 11s - 26ms/step - accuracy: 0.9465 - loss: 0.1842 - val_accuracy: 0.9493 - val_loss: 0.1870\n",
      "Epoch 7/20\n",
      "422/422 - 8s - 18ms/step - accuracy: 0.9521 - loss: 0.1662 - val_accuracy: 0.9493 - val_loss: 0.1703\n",
      "Epoch 8/20\n",
      "422/422 - 5s - 11ms/step - accuracy: 0.9573 - loss: 0.1473 - val_accuracy: 0.9615 - val_loss: 0.1379\n",
      "Epoch 9/20\n",
      "422/422 - 5s - 11ms/step - accuracy: 0.9608 - loss: 0.1366 - val_accuracy: 0.9647 - val_loss: 0.1289\n",
      "Epoch 10/20\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9635 - loss: 0.1248 - val_accuracy: 0.9640 - val_loss: 0.1246\n",
      "Epoch 11/20\n",
      "422/422 - 11s - 26ms/step - accuracy: 0.9654 - loss: 0.1195 - val_accuracy: 0.9662 - val_loss: 0.1135\n",
      "Epoch 12/20\n",
      "422/422 - 12s - 29ms/step - accuracy: 0.9677 - loss: 0.1119 - val_accuracy: 0.9720 - val_loss: 0.0982\n",
      "Epoch 13/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9692 - loss: 0.1051 - val_accuracy: 0.9677 - val_loss: 0.1077\n",
      "Epoch 14/20\n",
      "422/422 - 12s - 29ms/step - accuracy: 0.9706 - loss: 0.1004 - val_accuracy: 0.9723 - val_loss: 0.1023\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744ms/step - accuracy: 0.9727 - loss: 0.0948\n",
      "--- Starting trial: run-2\n",
      "{'filter_size': 5, 'optimizer': 'adam'}\n",
      "Epoch 1/20\n",
      "422/422 - 16s - 37ms/step - accuracy: 0.9294 - loss: 0.2476 - val_accuracy: 0.9785 - val_loss: 0.0738\n",
      "Epoch 2/20\n",
      "422/422 - 13s - 31ms/step - accuracy: 0.9784 - loss: 0.0707 - val_accuracy: 0.9857 - val_loss: 0.0507\n",
      "Epoch 3/20\n",
      "422/422 - 14s - 32ms/step - accuracy: 0.9857 - loss: 0.0472 - val_accuracy: 0.9892 - val_loss: 0.0393\n",
      "Epoch 4/20\n",
      "422/422 - 14s - 33ms/step - accuracy: 0.9886 - loss: 0.0365 - val_accuracy: 0.9897 - val_loss: 0.0356\n",
      "Epoch 5/20\n",
      "422/422 - 13s - 32ms/step - accuracy: 0.9904 - loss: 0.0319 - val_accuracy: 0.9953 - val_loss: 0.0172\n",
      "Epoch 6/20\n",
      "422/422 - 14s - 33ms/step - accuracy: 0.9923 - loss: 0.0248 - val_accuracy: 0.9910 - val_loss: 0.0285\n",
      "Epoch 7/20\n",
      "422/422 - 7s - 16ms/step - accuracy: 0.9936 - loss: 0.0216 - val_accuracy: 0.9950 - val_loss: 0.0179\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - accuracy: 0.9906 - loss: 0.0259\n",
      "--- Starting trial: run-3\n",
      "{'filter_size': 5, 'optimizer': 'sgd'}\n",
      "Epoch 1/20\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.6985 - loss: 1.2101 - val_accuracy: 0.8948 - val_loss: 0.3920\n",
      "Epoch 2/20\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9109 - loss: 0.3089 - val_accuracy: 0.9273 - val_loss: 0.2453\n",
      "Epoch 3/20\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9330 - loss: 0.2291 - val_accuracy: 0.9442 - val_loss: 0.1962\n",
      "Epoch 4/20\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9456 - loss: 0.1847 - val_accuracy: 0.9497 - val_loss: 0.1611\n",
      "Epoch 5/20\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9541 - loss: 0.1566 - val_accuracy: 0.9527 - val_loss: 0.1533\n",
      "Epoch 6/20\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.9610 - loss: 0.1357 - val_accuracy: 0.9595 - val_loss: 0.1290\n",
      "Epoch 7/20\n",
      "422/422 - 6s - 14ms/step - accuracy: 0.9649 - loss: 0.1214 - val_accuracy: 0.9593 - val_loss: 0.1279\n",
      "Epoch 8/20\n",
      "422/422 - 6s - 13ms/step - accuracy: 0.9670 - loss: 0.1099 - val_accuracy: 0.9695 - val_loss: 0.1060\n",
      "Epoch 9/20\n",
      "422/422 - 6s - 14ms/step - accuracy: 0.9700 - loss: 0.1018 - val_accuracy: 0.9772 - val_loss: 0.0870\n",
      "Epoch 10/20\n",
      "422/422 - 6s - 15ms/step - accuracy: 0.9715 - loss: 0.0953 - val_accuracy: 0.9707 - val_loss: 0.0941\n",
      "Epoch 11/20\n",
      "422/422 - 7s - 16ms/step - accuracy: 0.9732 - loss: 0.0907 - val_accuracy: 0.9725 - val_loss: 0.0866\n",
      "Epoch 12/20\n",
      "422/422 - 7s - 18ms/step - accuracy: 0.9745 - loss: 0.0843 - val_accuracy: 0.9768 - val_loss: 0.0786\n",
      "Epoch 13/20\n",
      "422/422 - 8s - 18ms/step - accuracy: 0.9755 - loss: 0.0816 - val_accuracy: 0.9760 - val_loss: 0.0810\n",
      "Epoch 14/20\n",
      "422/422 - 7s - 18ms/step - accuracy: 0.9772 - loss: 0.0769 - val_accuracy: 0.9735 - val_loss: 0.0788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 528ms/step - accuracy: 0.9787 - loss: 0.0743\n",
      "--- Starting trial: run-4\n",
      "{'filter_size': 7, 'optimizer': 'adam'}\n",
      "Epoch 1/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9339 - loss: 0.2371 - val_accuracy: 0.9762 - val_loss: 0.0784\n",
      "Epoch 2/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9786 - loss: 0.0696 - val_accuracy: 0.9820 - val_loss: 0.0541\n",
      "Epoch 3/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9853 - loss: 0.0489 - val_accuracy: 0.9875 - val_loss: 0.0433\n",
      "Epoch 4/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9887 - loss: 0.0376 - val_accuracy: 0.9923 - val_loss: 0.0261\n",
      "Epoch 5/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9907 - loss: 0.0305 - val_accuracy: 0.9893 - val_loss: 0.0313\n",
      "Epoch 6/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9921 - loss: 0.0264 - val_accuracy: 0.9937 - val_loss: 0.0209\n",
      "Epoch 7/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9932 - loss: 0.0222 - val_accuracy: 0.9967 - val_loss: 0.0155\n",
      "Epoch 8/20\n",
      "422/422 - 12s - 29ms/step - accuracy: 0.9951 - loss: 0.0175 - val_accuracy: 0.9955 - val_loss: 0.0133\n",
      "Epoch 9/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9946 - loss: 0.0168 - val_accuracy: 0.9960 - val_loss: 0.0119\n",
      "Epoch 10/20\n",
      "422/422 - 12s - 29ms/step - accuracy: 0.9954 - loss: 0.0147 - val_accuracy: 0.9963 - val_loss: 0.0121\n",
      "Epoch 11/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9963 - loss: 0.0121 - val_accuracy: 0.9953 - val_loss: 0.0134\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 753ms/step - accuracy: 0.9881 - loss: 0.0375\n",
      "--- Starting trial: run-5\n",
      "{'filter_size': 7, 'optimizer': 'sgd'}\n",
      "Epoch 1/20\n",
      "422/422 - 14s - 32ms/step - accuracy: 0.7290 - loss: 1.0409 - val_accuracy: 0.8895 - val_loss: 0.3914\n",
      "Epoch 2/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9110 - loss: 0.3176 - val_accuracy: 0.9220 - val_loss: 0.2712\n",
      "Epoch 3/20\n",
      "422/422 - 13s - 31ms/step - accuracy: 0.9304 - loss: 0.2420 - val_accuracy: 0.9343 - val_loss: 0.2311\n",
      "Epoch 4/20\n",
      "422/422 - 13s - 31ms/step - accuracy: 0.9411 - loss: 0.2029 - val_accuracy: 0.9460 - val_loss: 0.1863\n",
      "Epoch 5/20\n",
      "422/422 - 12s - 29ms/step - accuracy: 0.9496 - loss: 0.1757 - val_accuracy: 0.9527 - val_loss: 0.1770\n",
      "Epoch 6/20\n",
      "422/422 - 11s - 25ms/step - accuracy: 0.9552 - loss: 0.1568 - val_accuracy: 0.9572 - val_loss: 0.1464\n",
      "Epoch 7/20\n",
      "422/422 - 5s - 11ms/step - accuracy: 0.9580 - loss: 0.1417 - val_accuracy: 0.9562 - val_loss: 0.1518\n",
      "Epoch 8/20\n",
      "422/422 - 5s - 11ms/step - accuracy: 0.9607 - loss: 0.1322 - val_accuracy: 0.9645 - val_loss: 0.1213\n",
      "Epoch 9/20\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9647 - loss: 0.1222 - val_accuracy: 0.9638 - val_loss: 0.1232\n",
      "Epoch 10/20\n",
      "422/422 - 10s - 24ms/step - accuracy: 0.9669 - loss: 0.1133 - val_accuracy: 0.9647 - val_loss: 0.1195\n",
      "Epoch 11/20\n",
      "422/422 - 13s - 30ms/step - accuracy: 0.9691 - loss: 0.1073 - val_accuracy: 0.9708 - val_loss: 0.1041\n",
      "Epoch 12/20\n",
      "422/422 - 5s - 11ms/step - accuracy: 0.9697 - loss: 0.1019 - val_accuracy: 0.9733 - val_loss: 0.0910\n",
      "Epoch 13/20\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9719 - loss: 0.0957 - val_accuracy: 0.9743 - val_loss: 0.0863\n",
      "Epoch 14/20\n",
      "422/422 - 13s - 31ms/step - accuracy: 0.9728 - loss: 0.0926 - val_accuracy: 0.9742 - val_loss: 0.0908\n",
      "Epoch 15/20\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.9744 - loss: 0.0869 - val_accuracy: 0.9767 - val_loss: 0.0820\n",
      "Epoch 16/20\n",
      "422/422 - 5s - 11ms/step - accuracy: 0.9751 - loss: 0.0845 - val_accuracy: 0.9785 - val_loss: 0.0761\n",
      "Epoch 17/20\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9760 - loss: 0.0816 - val_accuracy: 0.9777 - val_loss: 0.0762\n",
      "Epoch 18/20\n",
      "422/422 - 9s - 21ms/step - accuracy: 0.9773 - loss: 0.0782 - val_accuracy: 0.9787 - val_loss: 0.0784\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774ms/step - accuracy: 0.9758 - loss: 0.0824\n"
     ]
    }
   ],
   "source": [
    "# Performing a grid search on the hyperparameters we need to test\n",
    "session_num = 0\n",
    "\n",
    "for filter_size in HP_FILTER_SIZE.domain.values:\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "    \n",
    "        hparams = {\n",
    "            HP_FILTER_SIZE: filter_size,\n",
    "            HP_OPTIMIZER: optimizer\n",
    "        }\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        run('logs/hparam_tuning/' + run_name, hparams)\n",
    "\n",
    "        session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the hyperparameter results with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 27552), started 0:23:32 ago. (Use '!kill 27552' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d3204acafa3b7fa8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d3204acafa3b7fa8\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the Tensorboard extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs/hparam_tuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    " %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. [Common techniques for better performance of neural networks ](#techniques)\n",
    "   - 8.1 [Regularization](#regu)\n",
    "   - 8.2 [L2 Regularization and weight decay](#L2)\n",
    "   - 8.3 [Dropout](#dropout)\n",
    "   - 8.4 [Data Augmentation](#dataaugmentatio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 [Regularization](#regu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ere](regularization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ret](reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 [L2 Regularization and weight decay](#L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularization in Machine Learning\n",
    "\n",
    "In machine learning, **regularization** is a technique used to prevent overfitting by adding a penalty term to the loss function. This penalty term discourages complex models, pushing them to be simpler. There are two common types of regularization: **L1 (Lasso)** and **L2 (Ridge)** regularization.\n",
    "\n",
    "## L1 Regularization (Lasso)\n",
    "- L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients.\n",
    "- This is represented as:\n",
    "\n",
    " $$\n",
    "  \\lambda \\sum |w|\n",
    " $$\n",
    "\n",
    "  where $ w $ represents the model weights, and $ \\lambda $ is the regularization parameter.\n",
    "- It encourages sparse models where only the most important features have non-zero weights, effectively performing feature selection by setting some weights to zero.\n",
    "\n",
    "## L2 Regularization (Ridge)\n",
    "- L2 regularization adds a penalty equal to the square of the magnitude of coefficients.\n",
    "- This is represented as:\n",
    "\n",
    " $$\n",
    "  \\lambda \\sum w^2\n",
    " $$\n",
    "\n",
    "- It discourages large weights but, unlike L1, does not force weights to be zero. Instead, it encourages smaller weights, helping to reduce model complexity without dropping features entirely.\n",
    "\n",
    "---\n",
    "\n",
    "Both methods aim to balance model complexity and accuracy by adjusting the $ \\lambda $ parameter. Higher $ \\lambda $ values mean more regularization and simpler models, while lower values allow more complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rte](reg2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fge](decay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fsdsd](decay2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 [Dropout](#dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout in Machine Learning\n",
    "\n",
    "**Dropout** is a regularization technique used to prevent overfitting in neural networks. It works by randomly \"dropping out\" a set of neurons during each training iteration. This effectively prevents the network from becoming too reliant on any single neuron, encouraging it to learn more robust features across all neurons.\n",
    "\n",
    "## How Dropout Works\n",
    "- During each training iteration, dropout randomly sets a percentage of neurons to zero.\n",
    "- This \"drops out\" a fraction of the network connections, creating a thinner network at each step and forces the remaining neurons to learn more generalized features.\n",
    "\n",
    "## Benefits of Dropout\n",
    "- Reduces overfitting by ensuring that no single neuron becomes overly specialized.\n",
    "- Helps the network generalize better, as it learns to make predictions without relying on any specific neurons.\n",
    "- During inference (prediction), dropout is disabled, but the learned weights are scaled down to adjust for the training phase's dropout effect.\n",
    "\n",
    "The **dropout rate** (usually between 0.2 and 0.5) determines the fraction of neurons to drop in each layer. Higher dropout rates increase regularization, while lower dropout rates retain more connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 8.4 [Data Augmentation](#dataaugmentatio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation in Machine Learning\n",
    "\n",
    "**Data Augmentation** is a technique used to artificially increase the size of a training dataset by creating modified versions of existing data samples. It is especially popular in computer vision and natural language processing to improve model generalization and reduce overfitting by introducing more variation.\n",
    "\n",
    "## How Data Augmentation Works\n",
    "- By applying various transformations (e.g., rotation, scaling, flipping, cropping), data augmentation creates altered versions of the original data.\n",
    "- For image data, augmentations can include flipping, rotation, brightness adjustment, zooming, and even color shifts.\n",
    "- In NLP, augmentations can involve synonym replacement, sentence reordering, and noise injection.\n",
    "\n",
    "## Benefits of Data Augmentation\n",
    "- Expands the effective size of the training dataset without collecting new data.\n",
    "- Helps the model learn more generalized features by training on diverse representations of data.\n",
    "- Reduces overfitting, as the model is exposed to a greater variety of input variations.\n",
    "\n",
    "Data augmentation is widely used in deep learning to improve a model’s robustness by exposing it to scenarios it may encounter in real-world data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dsfdf](aug.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 9. [A practical project: Labelling fashion items](#example)\n",
    "     - 9.1 [Introduction to the problem](#intro)\n",
    "     - 9.2 [The objective and the image]($objective)\n",
    "     - 9.3 [Converting images to array](#array)\n",
    "     - 9.4 [Coding](#code)\n",
    "     - 9.5 [Classification](#classification)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 [Introduction to the problem](#intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Clothing Image Classification\n",
    "\n",
    "Clothing image classification is a popular computer vision problem where the goal is to classify images of clothing into predefined categories, such as shirts, pants, dresses, or shoes. This task can be complex, as clothing items vary widely in shape, color, texture, and style. The problem is commonly approached with machine learning and deep learning techniques, particularly with convolutional neural networks (CNNs) due to their effectiveness in image processing tasks.\n",
    "\n",
    "## Problem Definition\n",
    "- Given an input image of clothing, the model needs to predict the category or label that best describes the item.\n",
    "- For example, if the image shows a T-shirt, the model should output \"T-shirt\" as the label.\n",
    "\n",
    "## Dataset\n",
    "- A popular dataset for this problem is the **Fashion MNIST** dataset, which contains 70,000 grayscale images of clothing items, each labeled with one of 10 categories, including T-shirt, Trouser, Pullover, Dress, and more.\n",
    "- Images in Fashion MNIST are 28x28 pixels, making them ideal for experimenting with small convolutional networks and deep learning techniques without requiring large computational resources.\n",
    "\n",
    "## Challenges\n",
    "- **Intra-class variability**: Clothes in the same category (e.g., dresses) can look very different due to variations in style, pattern, and fabric.\n",
    "- **Inter-class similarity**: Some categories may look similar (e.g., a T-shirt vs. a pullover), making it challenging for the model to distinguish between them.\n",
    "- **Lighting and background variations**: Real-world images often have differences in lighting and backgrounds, which can impact the model's performance.\n",
    "\n",
    "## Approaches\n",
    "- **Convolutional Neural Networks (CNNs)**: CNNs are commonly used for clothing classification tasks due to their ability to learn spatial hierarchies of features, such as edges, shapes, and textures, which are crucial for identifying clothing types.\n",
    "- **Data Augmentation**: Techniques like flipping, rotation, and scaling help increase dataset variability and reduce overfitting, enabling the model to generalize better to unseen images.\n",
    "\n",
    "## Applications\n",
    "- **E-commerce**: Automatically tagging clothing items for cataloging and searching.\n",
    "- **Fashion analytics**: Analyzing popular clothing trends based on user-uploaded images.\n",
    "- **Virtual fitting rooms**: Providing personalized recommendations for customers by analyzing their style.\n",
    "\n",
    "By training a model on labeled clothing images, we can improve its ability to recognize and categorize various types of clothing, helping advance practical applications in retail, fashion, and beyond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 9.2 [The objective and the image]($objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6](91.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![r](92.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![r1](93.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rtr](94.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 9.3 [Converting images to array](#array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the image file we will use\n",
    "filename = \"C:/Users/usuario/Desktop/Doctorado/Programas en Python/proyecto fashion/Test image/Test image.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image as a Python variable\n",
    "# We can manipulate this variable with the Pillow package\n",
    "image = Image.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the image contained in the variable through your default photo veiwer\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the image while keeping the aspect ration constant\n",
    "# The 'image' variable is updated with the new image\n",
    "image.thumbnail((90,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the variable into a NumPy array (tensor) \n",
    "image_array = np.asarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 90, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dimensions of the array\n",
    "# The convention for image arrays is (height x width x channels)\n",
    "# 'channels' refferes to the colors \n",
    "#   - 1 for grayscale (only 1 value for each pixel)\n",
    "#   - 3 for color images (red, green and blue values/channels)\n",
    "np.shape(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 9.4 [Coding](#code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![trtrt](76.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "data_train = np.load(r\"C:/Users/usuario/Desktop/Doctorado/Programas en Python/proyecto fashion/Full Dataset/Primary categories - Train.npz\")\n",
    "data_val = np.load(r\"C:/Users/usuario/Desktop/Doctorado/Programas en Python/proyecto fashion/Full Dataset/Primary categories - Validation.npz\")\n",
    "data_test = np.load(r\"C:/Users/usuario/Desktop/Doctorado/Programas en Python/proyecto fashion/Full Dataset/Primary categories - Test.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the arrays from the imported data\n",
    "images_train = data_train['images']\n",
    "labels_train = data_train['labels']\n",
    "\n",
    "images_val = data_val['images']\n",
    "labels_val = data_val['labels']\n",
    "\n",
    "images_test = data_test['images']\n",
    "labels_test = data_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the pixel values of all images\n",
    "images_train = images_train/255.0\n",
    "images_val = images_val/255.0\n",
    "images_test = images_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters we would tune, and their values to be tested\n",
    "HP_FILTER_SIZE = hp.HParam('filter_size', hp.Discrete([3,5,7]))\n",
    "HP_FILTER_NUM = hp.HParam('filters_number', hp.Discrete([32,64,96,128]))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "# Logging setup info\n",
    "with tf.summary.create_file_writer(r'Logs/Model 1/hparam_tuning/').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_FILTER_SIZE, HP_FILTER_NUM],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping our model and training in a function\n",
    "def train_test_model(hparams, session_num):\n",
    "    \n",
    "    # Outlining the model/architecture of our CNN\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(hparams[HP_FILTER_NUM], hparams[HP_FILTER_SIZE], activation='relu', input_shape=(120,90,3)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(hparams[HP_FILTER_NUM], 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(3)\n",
    "    ])\n",
    "    \n",
    "    # Defining the loss function\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    # Defining the logging directory\n",
    "    log_dir = \"Logs\\\\Model 1\\\\fit\\\\\" + \"run-{}\".format(session_num)\n",
    "    \n",
    "    \n",
    "    def plot_confusion_matrix(cm, class_names):\n",
    "        \"\"\"\n",
    "        Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "        Args:\n",
    "          cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "          class_names (array, shape = [n]): String names of the integer classes\n",
    "        \"\"\"\n",
    "        figure = plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title(\"Confusion matrix\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=45)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "\n",
    "        # Normalize the confusion matrix.\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "        # Use white text if squares are dark; otherwise black.\n",
    "        threshold = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        return figure\n",
    "    \n",
    "    \n",
    "    \n",
    "    def plot_to_image(figure):\n",
    "        \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "        returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "        # Save the plot to a PNG in memory.\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        # Closing the figure prevents it from being displayed directly inside\n",
    "        # the notebook.\n",
    "        plt.close(figure)\n",
    "        buf.seek(0)\n",
    "        # Convert PNG buffer to TF image\n",
    "        image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "        # Add the batch dimension\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        return image\n",
    "    \n",
    "    \n",
    "    # Defining a file writer for Confusion Matrix logging purposes\n",
    "    file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')     \n",
    "    \n",
    "    \n",
    "    def log_confusion_matrix(epoch, logs):\n",
    "        # Use the model to predict the values from the validation dataset.\n",
    "        test_pred_raw = model.predict(images_val)\n",
    "        test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "        # Calculate the confusion matrix.\n",
    "        cm = sklearn.metrics.confusion_matrix(labels_val, test_pred)\n",
    "        # Log the confusion matrix as an image summary.\n",
    "        figure = plot_confusion_matrix(cm, class_names=['Glasses/Sunglasses', 'Trousers/Jeans', 'Shoes'])\n",
    "        cm_image = plot_to_image(figure)\n",
    "\n",
    "        # Log the confusion matrix as an image summary.\n",
    "        with file_writer_cm.as_default():\n",
    "            tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the Tensorboard and Confusion Matrix callbacks.\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\n",
    "    cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "\n",
    "    \n",
    "    # Defining early stopping to prevent overfitting\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        mode = 'auto',\n",
    "        min_delta = 0,\n",
    "        patience = 2,\n",
    "        verbose = 0, \n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    \n",
    "    # Training the model\n",
    "    model.fit(\n",
    "        images_train,\n",
    "        labels_train,\n",
    "        epochs = EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        callbacks = [tensorboard_callback, cm_callback, early_stopping],\n",
    "        validation_data = (images_val,labels_val),\n",
    "        verbose = 2\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Evaluating the model's performance on the validation set\n",
    "    _, accuracy = model.evaluate(images_val,labels_val)\n",
    "    \n",
    "    # Saving the current model for future reference\n",
    "    model.save(r\"saved_models\\Model 1\\Run-{}.h5\".format(session_num))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to log the resuls\n",
    "def run(log_dir, hparams, session_num):\n",
    "    \n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(hparams, session_num)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-1\n",
      "{'filter_size': 3, 'filters_number': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "203/203 - 13s - 66ms/step - accuracy: 0.9794 - loss: 0.0831 - val_accuracy: 0.9994 - val_loss: 0.0050\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "203/203 - 11s - 53ms/step - accuracy: 0.9985 - loss: 0.0114 - val_accuracy: 0.9981 - val_loss: 0.0052\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "203/203 - 10s - 51ms/step - accuracy: 0.9982 - loss: 0.0114 - val_accuracy: 0.9994 - val_loss: 0.0022\n",
      "Epoch 4/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "203/203 - 11s - 52ms/step - accuracy: 0.9988 - loss: 0.0081 - val_accuracy: 0.9988 - val_loss: 0.0046\n",
      "Epoch 5/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "203/203 - 12s - 57ms/step - accuracy: 0.9993 - loss: 0.0050 - val_accuracy: 0.9981 - val_loss: 0.0065\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-2\n",
      "{'filter_size': 3, 'filters_number': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "203/203 - 30s - 148ms/step - accuracy: 0.9822 - loss: 0.0617 - val_accuracy: 0.9975 - val_loss: 0.0100\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "203/203 - 27s - 134ms/step - accuracy: 0.9980 - loss: 0.0162 - val_accuracy: 0.9981 - val_loss: 0.0121\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "203/203 - 27s - 135ms/step - accuracy: 0.9985 - loss: 0.0116 - val_accuracy: 0.9981 - val_loss: 0.0051\n",
      "Epoch 4/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "203/203 - 28s - 138ms/step - accuracy: 0.9990 - loss: 0.0077 - val_accuracy: 0.9981 - val_loss: 0.0072\n",
      "Epoch 5/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "203/203 - 27s - 134ms/step - accuracy: 0.9995 - loss: 0.0050 - val_accuracy: 0.9994 - val_loss: 0.0046\n",
      "Epoch 6/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "203/203 - 28s - 139ms/step - accuracy: 0.9993 - loss: 0.0045 - val_accuracy: 0.9852 - val_loss: 0.0451\n",
      "Epoch 7/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "203/203 - 29s - 144ms/step - accuracy: 0.9968 - loss: 0.0144 - val_accuracy: 0.9994 - val_loss: 0.0011\n",
      "Epoch 8/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "203/203 - 27s - 135ms/step - accuracy: 0.9998 - loss: 0.0025 - val_accuracy: 0.9981 - val_loss: 0.0040\n",
      "Epoch 9/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "203/203 - 28s - 137ms/step - accuracy: 0.9997 - loss: 0.0020 - val_accuracy: 0.9988 - val_loss: 0.0016\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9997 - loss: 7.3499e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-3\n",
      "{'filter_size': 3, 'filters_number': 96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
      "203/203 - 55s - 272ms/step - accuracy: 0.9773 - loss: 0.0883 - val_accuracy: 0.9981 - val_loss: 0.0084\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step\n",
      "203/203 - 63s - 312ms/step - accuracy: 0.9958 - loss: 0.0240 - val_accuracy: 0.9988 - val_loss: 0.0041\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
      "203/203 - 51s - 252ms/step - accuracy: 0.9991 - loss: 0.0095 - val_accuracy: 0.9981 - val_loss: 0.0097\n",
      "Epoch 4/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
      "203/203 - 54s - 264ms/step - accuracy: 0.9930 - loss: 0.0289 - val_accuracy: 0.9975 - val_loss: 0.0094\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9987 - loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-4\n",
      "{'filter_size': 3, 'filters_number': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 67ms/step\n",
      "203/203 - 104s - 511ms/step - accuracy: 0.9731 - loss: 0.1086 - val_accuracy: 0.9346 - val_loss: 0.1150\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step\n",
      "203/203 - 92s - 454ms/step - accuracy: 0.9964 - loss: 0.0258 - val_accuracy: 0.9969 - val_loss: 0.0087\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 138ms/step\n",
      "203/203 - 199s - 982ms/step - accuracy: 0.9991 - loss: 0.0098 - val_accuracy: 0.9969 - val_loss: 0.0146\n",
      "Epoch 4/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step\n",
      "203/203 - 194s - 955ms/step - accuracy: 0.9994 - loss: 0.0068 - val_accuracy: 0.9994 - val_loss: 0.0024\n",
      "Epoch 5/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 136ms/step\n",
      "203/203 - 211s - 1s/step - accuracy: 0.9996 - loss: 0.0042 - val_accuracy: 0.9994 - val_loss: 0.0027\n",
      "Epoch 6/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step\n",
      "203/203 - 171s - 842ms/step - accuracy: 0.9997 - loss: 0.0037 - val_accuracy: 0.9988 - val_loss: 0.0025\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.9997 - loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-5\n",
      "{'filter_size': 5, 'filters_number': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "203/203 - 14s - 70ms/step - accuracy: 0.9740 - loss: 0.0819 - val_accuracy: 0.9969 - val_loss: 0.0076\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "203/203 - 13s - 62ms/step - accuracy: 0.9981 - loss: 0.0164 - val_accuracy: 0.9981 - val_loss: 0.0057\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "203/203 - 13s - 65ms/step - accuracy: 0.9989 - loss: 0.0100 - val_accuracy: 0.9988 - val_loss: 0.0067\n",
      "Epoch 4/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "203/203 - 13s - 65ms/step - accuracy: 0.9988 - loss: 0.0086 - val_accuracy: 0.9981 - val_loss: 0.0099\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9986 - loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-6\n",
      "{'filter_size': 5, 'filters_number': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "203/203 - 36s - 175ms/step - accuracy: 0.9742 - loss: 0.0872 - val_accuracy: 0.9988 - val_loss: 0.0082\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "203/203 - 42s - 205ms/step - accuracy: 0.9984 - loss: 0.0159 - val_accuracy: 0.9981 - val_loss: 0.0113\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "203/203 - 32s - 158ms/step - accuracy: 0.9785 - loss: 0.0656 - val_accuracy: 0.9963 - val_loss: 0.0122\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9996 - loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-7\n",
      "{'filter_size': 5, 'filters_number': 96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step\n",
      "203/203 - 66s - 323ms/step - accuracy: 0.9733 - loss: 0.0799 - val_accuracy: 0.9988 - val_loss: 0.0079\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
      "203/203 - 60s - 293ms/step - accuracy: 0.9983 - loss: 0.0156 - val_accuracy: 0.9975 - val_loss: 0.0180\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 111ms/step\n",
      "203/203 - 95s - 466ms/step - accuracy: 0.9944 - loss: 0.0240 - val_accuracy: 0.9981 - val_loss: 0.0088\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - accuracy: 0.9996 - loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-8\n",
      "{'filter_size': 5, 'filters_number': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step\n",
      "203/203 - 215s - 1s/step - accuracy: 0.9709 - loss: 0.0940 - val_accuracy: 0.9969 - val_loss: 0.0118\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 136ms/step\n",
      "203/203 - 208s - 1s/step - accuracy: 0.9975 - loss: 0.0186 - val_accuracy: 0.9981 - val_loss: 0.0066\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 69ms/step\n",
      "203/203 - 95s - 470ms/step - accuracy: 0.9984 - loss: 0.0118 - val_accuracy: 0.9981 - val_loss: 0.0048\n",
      "Epoch 4/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step\n",
      "203/203 - 99s - 486ms/step - accuracy: 0.9991 - loss: 0.0096 - val_accuracy: 0.9981 - val_loss: 0.0125\n",
      "Epoch 5/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step\n",
      "203/203 - 98s - 481ms/step - accuracy: 0.9981 - loss: 0.0082 - val_accuracy: 0.9981 - val_loss: 0.0104\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - accuracy: 0.9986 - loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-9\n",
      "{'filter_size': 7, 'filters_number': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
      "203/203 - 18s - 90ms/step - accuracy: 0.9788 - loss: 0.0763 - val_accuracy: 0.9981 - val_loss: 0.0099\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "203/203 - 15s - 73ms/step - accuracy: 0.9980 - loss: 0.0145 - val_accuracy: 0.9981 - val_loss: 0.0072\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "203/203 - 14s - 70ms/step - accuracy: 0.9992 - loss: 0.0092 - val_accuracy: 0.9988 - val_loss: 0.0110\n",
      "Epoch 4/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "203/203 - 14s - 71ms/step - accuracy: 0.9968 - loss: 0.0137 - val_accuracy: 0.9957 - val_loss: 0.0246\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9989 - loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-10\n",
      "{'filter_size': 7, 'filters_number': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step\n",
      "203/203 - 38s - 186ms/step - accuracy: 0.9694 - loss: 0.1008 - val_accuracy: 0.9981 - val_loss: 0.0134\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "203/203 - 35s - 172ms/step - accuracy: 0.9969 - loss: 0.0193 - val_accuracy: 0.9932 - val_loss: 0.0177\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
      "203/203 - 35s - 170ms/step - accuracy: 0.9981 - loss: 0.0139 - val_accuracy: 0.9981 - val_loss: 0.0074\n",
      "Epoch 4/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
      "203/203 - 35s - 170ms/step - accuracy: 0.9986 - loss: 0.0099 - val_accuracy: 0.9988 - val_loss: 0.0041\n",
      "Epoch 5/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step\n",
      "203/203 - 34s - 169ms/step - accuracy: 0.9992 - loss: 0.0050 - val_accuracy: 0.9994 - val_loss: 0.0032\n",
      "Epoch 6/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "203/203 - 35s - 172ms/step - accuracy: 0.9991 - loss: 0.0053 - val_accuracy: 0.9981 - val_loss: 0.0089\n",
      "Epoch 7/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "203/203 - 35s - 171ms/step - accuracy: 0.9985 - loss: 0.0083 - val_accuracy: 0.9988 - val_loss: 0.0047\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9997 - loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-11\n",
      "{'filter_size': 7, 'filters_number': 96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step\n",
      "203/203 - 65s - 321ms/step - accuracy: 0.9686 - loss: 0.1128 - val_accuracy: 0.9975 - val_loss: 0.0128\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step\n",
      "203/203 - 66s - 325ms/step - accuracy: 0.9980 - loss: 0.0153 - val_accuracy: 0.9975 - val_loss: 0.0081\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step\n",
      "203/203 - 65s - 319ms/step - accuracy: 0.9987 - loss: 0.0103 - val_accuracy: 0.9988 - val_loss: 0.0052\n",
      "Epoch 4/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step\n",
      "203/203 - 63s - 311ms/step - accuracy: 0.9990 - loss: 0.0080 - val_accuracy: 0.9981 - val_loss: 0.0059\n",
      "Epoch 5/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step\n",
      "203/203 - 63s - 310ms/step - accuracy: 0.9992 - loss: 0.0055 - val_accuracy: 0.9963 - val_loss: 0.0123\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.9996 - loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-12\n",
      "{'filter_size': 7, 'filters_number': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step\n",
      "203/203 - 106s - 524ms/step - accuracy: 0.9570 - loss: 0.1246 - val_accuracy: 0.9988 - val_loss: 0.0071\n",
      "Epoch 2/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step\n",
      "203/203 - 102s - 500ms/step - accuracy: 0.9981 - loss: 0.0158 - val_accuracy: 0.9981 - val_loss: 0.0094\n",
      "Epoch 3/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step\n",
      "203/203 - 103s - 506ms/step - accuracy: 0.9985 - loss: 0.0111 - val_accuracy: 0.9988 - val_loss: 0.0039\n",
      "Epoch 4/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step\n",
      "203/203 - 100s - 494ms/step - accuracy: 0.9991 - loss: 0.0079 - val_accuracy: 0.9969 - val_loss: 0.0158\n",
      "Epoch 5/15\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step\n",
      "203/203 - 101s - 497ms/step - accuracy: 0.9982 - loss: 0.0080 - val_accuracy: 0.9914 - val_loss: 0.0275\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.9987 - loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "session_num = 1\n",
    "\n",
    "for filter_size in HP_FILTER_SIZE.domain.values:\n",
    "    for filter_num in HP_FILTER_NUM.domain.values:\n",
    "\n",
    "        hparams = {\n",
    "            HP_FILTER_SIZE: filter_size,\n",
    "            HP_FILTER_NUM: filter_num\n",
    "        }\n",
    "\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        run('Logs/Model 1/hparam_tuning/' + run_name, hparams, session_num)\n",
    "\n",
    "        session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Loading a model to evaluate on the test set\n",
    "model = tf.keras.models.load_model(r\"saved_models\\Model 1\\Run-1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0181\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(images_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0197. Test accuracy: 99.88%\n"
     ]
    }
   ],
   "source": [
    "# Printing the test results\n",
    "print('Test loss: {0:.4f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
